{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania - metodologia testowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walidacja krzyżowa\n",
    "\n",
    "1. (15 pkt) Zaimplementuj 4-krotną walidację krzyżową na danych treningowych ze zbioru MNIST. W jej wyniku powinny powstać cztery różne modele $M_i$ dla $i=1\\dots4$. Dla każdego modelu dobierz wspólne początkowe parametry ($\\alpha$, rozmiar wsadu, liczba epok), zastosuj randomizację danych między epokami.\n",
    "1. (10 pkt) Oblicz jakość każdego modelu na jego zbiorze walidacyjnym oraz średnią poprawność klasyfikacyjną na całym zbiorze treningowym poprzez uśrednienie wyników każdego z czterech zbiorów walidacyjnych.\n",
    "1. (15 pkt) Dobierz najlepsze parametry dla modeli $M_i$ na podstawie skuteczności osiąganej na zbiorze treningowym. Podaj skuteczności, podobnie jak w punkcie 2. \n",
    "1. (10 pkt) Zbuduj model z tak dobranymi parametrami (uśrednij, jeżeli są różne dla $M_i$) na całych danych treningowych i sprawdź jego skuteczność na zbiorze testowym. Czy model z takimi parametrami da najlepszy możliwy wynik na zbiorze testowym? Odpowiedź uzasadnij."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pprint as pp\n",
    "\n",
    "def safeSigmoid(x, eps=0):\n",
    "    y = 1.0/(1.0 + np.exp(-x))\n",
    "    if eps > 0:\n",
    "        y[y < eps] = eps\n",
    "        y[y > 1 - eps] = 1 - eps\n",
    "    return y\n",
    "\n",
    "def h(theta, X, eps=0.0):\n",
    "    return safeSigmoid(X*theta, eps)\n",
    "\n",
    "def J(h,theta,X,y):\n",
    "    m = len(y)\n",
    "    f = h(theta, X, eps=10**-7)\n",
    "    return -np.sum(np.multiply(y, np.log(f)) + \n",
    "                   np.multiply(1 - y, np.log(1 - f)), axis=0)/m\n",
    "\n",
    "def dJ(h,theta,X,y):\n",
    "    return 1.0/len(y)*(X.T*(h(theta,X)-y))\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X)/np.sum(np.exp(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SGD(h, fJ, fdJ, theta, X, y, \n",
    "        alpha=0.001, maxEpochs=1, batchSize=100, shuffle=False):\n",
    "    m, n = X.shape\n",
    "    start, end = 0, batchSize\n",
    "    \n",
    "    maxSteps = (m * float(maxEpochs)) / batchSize\n",
    "    for i in range(int(maxSteps)):\n",
    "        if shuffle and (i * batchSize) % m == 0:\n",
    "            indexes = [j for j in range(len(X))]\n",
    "            np.random.shuffle(indexes)\n",
    "            X = X[indexes]\n",
    "            y = y[indexes]\n",
    "            \n",
    "        XBatch, yBatch =  X[start:end,:], y[start:end,:]\n",
    "\n",
    "        theta = theta - alpha * fdJ(h, theta, XBatch, yBatch)\n",
    "        \n",
    "        if start + batchSize < m:\n",
    "            start += batchSize\n",
    "        else:\n",
    "            start = 0\n",
    "        end = min(start + batchSize, m)\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import itertools\n",
    "from statistics import mode\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def readMnist(dataset = \"training\", path = \".\"):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set.  It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "\n",
    "    get_img = lambda idx: (lbl[idx], img[idx])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_img(i)\n",
    "\n",
    "def showImage(image):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 2D array of pixel data.\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()\n",
    "    \n",
    "def mnistMatrix(data, maxItems=1000):\n",
    "    datalist = [t for t in data]\n",
    "    m = maxItems if maxItems != -1 else len(datalist)\n",
    "    n = 28 * 28 + 1\n",
    "    X = np.matrix(np.zeros(m * n)).reshape(m, n)\n",
    "    Y = np.matrix(np.zeros(m)).reshape(m, 1)\n",
    "    for i, (label, image) in enumerate(datalist[:m]):\n",
    "        X[i, 0] = 1 # bias term\n",
    "        X[i, 1:] = image.reshape(28*28,)\n",
    "        Y[i] = label\n",
    "    return X, Y\n",
    "\n",
    "def normalize(X):\n",
    "    X[:,1:] = X[:,1:] / 255\n",
    "    return X\n",
    "\n",
    "def mapY(y, cls):\n",
    "    m = len(y)\n",
    "    yBi = np.matrix(np.zeros(m)).reshape(m, 1)\n",
    "    yBi[y == cls] = 1.\n",
    "    return yBi\n",
    "\n",
    "def indicatorMatrix(y):\n",
    "    classes = np.unique(y.tolist())\n",
    "    m = len(y)\n",
    "    k = len(classes)\n",
    "    Y = np.matrix(np.zeros((m, k)))\n",
    "    for i, cls in enumerate(classes):\n",
    "        Y[:,i] = mapY(y, cls)\n",
    "    return Y\n",
    "\n",
    "def classify(thetas, X, debug=False):\n",
    "    regs = np.array([(X*theta).item() for theta in thetas])\n",
    "    if debug:\n",
    "        print(\"regs  =\", regs)\n",
    "    probs = softmax(regs)\n",
    "    if debug:\n",
    "        print(\"probs =\", np.around(probs, decimals=3))\n",
    "    return np.argmax(probs), probs\n",
    "\n",
    "def trainMaxEnt(X, Y, alpha = 1.0, maxEpochs = 2, batchSize = 50, shuffle = False):\n",
    "    n = X.shape[1]\n",
    "    thetas = []\n",
    "    for c in range(Y.shape[1]):\n",
    "        YBi = Y[:,c]\n",
    "        theta = np.matrix(np.random.random(n)).reshape(n,1)\n",
    "        thetaBest = SGD(h, J, dJ, theta, X, YBi, alpha, maxEpochs, batchSize, shuffle)\n",
    "        thetas.append(thetaBest)\n",
    "    return thetas\n",
    "\n",
    "def calculateAcc(thetas, X_test, y_test, debug=True):\n",
    "    acc = 0.0\n",
    "    for i in range(len(y_test)):\n",
    "        cls, probs = classify(thetas, X_test[i])\n",
    "        correctCls = int(y_test[i].item())\n",
    "        \n",
    "        acc += correctCls == cls\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walidacja krzyżowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_into_parts(X, y, amount):\n",
    "    chunk = int(round(len(X)/amount))\n",
    "    X_parts = [X[i:i+chunk] for i in range(0, len(X), chunk)]\n",
    "    y_parts = [y[i:i+chunk] for i in range(0, len(y), chunk)]\n",
    "    \n",
    "    return X_parts, y_parts\n",
    "\n",
    "X, y = mnistMatrix(readMnist(dataset='training'), maxItems=-1)\n",
    "X_test, y_test = mnistMatrix(readMnist(dataset='testing'), maxItems=-1)\n",
    "X_norm = normalize(X)\n",
    "X_test_norm = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy model 0 = 0.849200\n",
      "Accuracy model 1 = 0.854067\n",
      "Accuracy model 2 = 0.853400\n",
      "Accuracy model 3 = 0.852267\n"
     ]
    }
   ],
   "source": [
    "indexes = [j for j in range(len(X))]\n",
    "np.random.shuffle(indexes)\n",
    "X_norm = X_norm[indexes]\n",
    "y = y[indexes]\n",
    "\n",
    "X_parts, y_parts = divide_into_parts(X_norm, y, 4)\n",
    "thetas = []\n",
    "results = []\n",
    "for i in range(len(X_parts)):\n",
    "    D_x = X_parts[i]\n",
    "    D_y = y_parts[i]\n",
    "    S_x = np.concatenate([X_parts[j] for j in range(len(X_parts)) if j != i])\n",
    "    S_y = indicatorMatrix(np.concatenate([y_parts[j] for j in range(len(y_parts)) if j != i]))\n",
    "    theta = trainMaxEnt(S_x, S_y, alpha = 0.1, maxEpochs = 1, batchSize = 100, shuffle = True)\n",
    "    acc = calculateAcc(theta, D_x, D_y)/len(D_x)\n",
    "    print(\"Accuracy model %d = %f\" % (i, acc))\n",
    "    thetas.append(theta)\n",
    "    results.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total model = 0.852267\n"
     ]
    }
   ],
   "source": [
    "acc_mean = np.mean(results)\n",
    "print(\"Accuracy total model = %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy for model 0 and parameters (1.0, 3, 100) = 0.908400\n",
      "Best accuracy for model 1 and parameters (0.5, 2, 20) = 0.906667\n",
      "Best accuracy for model 2 and parameters (0.5, 2, 20) = 0.905000\n",
      "Best accuracy for model 3 and parameters (0.1, 3, 20) = 0.902533\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.1, 0.5, 1.0, 2.0]\n",
    "epochs = [2, 3, 1]\n",
    "batches = [50, 100, 10, 20]\n",
    "\n",
    "parameters = list(itertools.product(alphas, epochs, batches))\n",
    "best_parameters = []\n",
    "\n",
    "for i in range(len(X_parts)):\n",
    "    D_x = X_parts[i]\n",
    "    D_y = y_parts[i]\n",
    "    S_x = np.concatenate([X_parts[j] for j in range(len(X_parts)) if j != i])\n",
    "    S_y = indicatorMatrix(np.concatenate([y_parts[j] for j in range(len(y_parts)) if j != i]))\n",
    "    results = []\n",
    "\n",
    "    for parameter in parameters:\n",
    "        alpha, epoch, batch_size = parameter\n",
    "        theta = trainMaxEnt(S_x, S_y, alpha = alpha, maxEpochs = epoch, batchSize = batch_size, shuffle = True)\n",
    "        results.append((parameter, calculateAcc(theta, D_x, D_y)/len(D_x)))\n",
    "    \n",
    "    best_result = max(results, key = lambda item:item[1])\n",
    "    best_parameters.append(best_result[0])\n",
    "    print(\"Best accuracy for model %d and parameters %s = %f\" % (i, str(best_result[0]), best_result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total model for parameters (0.52500000000000002, 2, 40) = 0.911800\n"
     ]
    }
   ],
   "source": [
    "mean_parameters = np.mean(best_parameters, axis=0)\n",
    "alpha, epoch, batch = mean_parameters\n",
    "epoch, batch = int(epoch), int(batch)\n",
    "y_ind = indicatorMatrix(y)\n",
    "\n",
    "theta = trainMaxEnt(X_norm, y_ind, alpha = alpha, maxEpochs = epoch, batchSize = batch, shuffle = True)\n",
    "print(\"Accuracy total model for parameters %s = %f\" % ((alpha, epoch, batch), calculateAcc(theta, X_test_norm, y_test)/len(X_test_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie wydaje mi się, aby tak zbudowany model był najlepszy możliwy. A to dlatego, że nie zawsze uśrednianie parametrów będzie dawało najlepszą skuteczość. Łatwo wyobrazić sobie sytuację, gdy dla prawie wszystkich modeli najlepsze będą te same parametry, a dla jednego inne. Wówczas lepszą skuteczność uzyskalibyśmy biorąc te parametry, które w większości przypadków okazały się prowadzić do dobrego rozwiązania. Również zastosowanie średniej ważonej (wagi w zależności od liczby modeli, dla których dany zestaw parametrów okazał się najlepszy) byłoby lepszym pomysłem niż średnia arytmetyczna."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
