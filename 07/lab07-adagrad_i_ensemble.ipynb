{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania - metody optymalizacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 AdaGrad\n",
    "\n",
    "1. Rozszerz poniższą implementację algorytmu SGD o funkcjonalność algorytmu AdaGrad (włączaną parametrem `adaGrad=True`). Zasady działania AdaGrad są podane w materiałach do wykładu.\n",
    "   <br />**Uwaga**: podczas dzielenia przez pierwastek sumy kwadratów historycznych gradientów warto dodać małą wartość $\\epsilon=10^{-7}$ do mianownika, aby uniknąć dzielenia przez 0.\n",
    "   \n",
    "1. Stwórz model wieloklasowej regresji logistycznej na zbiorze MNIST (dla wszystkich 10 cyfr) i oblicz jego jakość na zbiorze testowym. Zastosuj poniższe parametry:\n",
    " 1. Rozmiar wsadu: 50\n",
    " 1. Liczba epok: 2\n",
    " 1. Rozmiar kroku $\\alpha$: 1.0 (w algorytmie AdaGrad $\\alpha$ może być niezmienne)\n",
    "1. Spróbuj uzyskać wynik podobny lub lepszy samym algorytmem SGD bez opcji AdaGrad, dostrajając parametry $\\alpha$. Czy jest to możliwe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Ensemble\n",
    "\n",
    "1. Na danych MNIST wytrenuj 10 klasyfikatorów z sensownie dobranymi parametrami dla algorytmu SGD. Zastosuj randomizację zbioru uczącego dla każdego modelu i w każdej epoce.<br />\n",
    "**Uwaga**: pamiętaj, aby tasować $X$ i $Y$ w tej samej kolejności! \n",
    "1. Oblicz jakość każdego z klasyfikatorów na zbiorze testowym.\n",
    "1. Oblicz jakość predykcji (klas) uzyskanych w wyniku wybranej metody agregacji wyników: głosowania klas lub uśredniania prawdopodobieństw, i porównaj z wcześniej uzyskanymi wynikami. Jak wynik z metody zbiorczej odnosi się do wyników uzyskiwanych przez pojedyncze klasyfikatory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import itertools\n",
    "from statistics import mode\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def readMnist(dataset = \"training\", path = \".\"):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set.  It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "\n",
    "    get_img = lambda idx: (lbl[idx], img[idx])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_img(i)\n",
    "\n",
    "def showImage(image):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 2D array of pixel data.\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnistMatrix(data, maxItems=1000):\n",
    "    datalist = [t for t in data]\n",
    "    m = maxItems if maxItems != -1 else len(datalist)\n",
    "    n = 28 * 28 + 1\n",
    "    X = np.matrix(np.zeros(m * n)).reshape(m, n)\n",
    "    Y = np.matrix(np.zeros(m)).reshape(m, 1)\n",
    "    for i, (label, image) in enumerate(datalist[:m]):\n",
    "        X[i, 0] = 1 # bias term\n",
    "        X[i, 1:] = image.reshape(28*28,)\n",
    "        Y[i] = label\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = mnistMatrix(readMnist(dataset='training'), maxItems=-1)\n",
    "X_test, y_test = mnistMatrix(readMnist(dataset='testing'), maxItems=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    X[:,1:] = X[:,1:] / 255\n",
    "    return X\n",
    "\n",
    "def mapY(y, cls):\n",
    "    m = len(y)\n",
    "    yBi = np.matrix(np.zeros(m)).reshape(m, 1)\n",
    "    yBi[y == cls] = 1.\n",
    "    return yBi\n",
    "\n",
    "def indicatorMatrix(y):\n",
    "    classes = np.unique(y.tolist())\n",
    "    m = len(y)\n",
    "    k = len(classes)\n",
    "    Y = np.matrix(np.zeros((m, k)))\n",
    "    for i, cls in enumerate(classes):\n",
    "        Y[:,i] = mapY(y, cls)\n",
    "    return Y\n",
    "\n",
    "def safeSigmoid(x, eps=0):\n",
    "    y = 1.0/(1.0 + np.exp(-x))\n",
    "    if eps > 0:\n",
    "        y[y < eps] = eps\n",
    "        y[y > 1 - eps] = 1 - eps\n",
    "    return y\n",
    "\n",
    "def h(theta, X, eps=0.0):\n",
    "    return safeSigmoid(X*theta, eps)\n",
    "\n",
    "def J(h,theta,X,y):\n",
    "    m = len(y)\n",
    "    f = h(theta, X, eps=10**-7)\n",
    "    return -np.sum(np.multiply(y, np.log(f)) + \n",
    "                   np.multiply(1 - y, np.log(1 - f)), axis=0)/m\n",
    "\n",
    "def dJ(h,theta,X,y):\n",
    "    return 1.0/len(y)*(X.T*(h(theta,X)-y))\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X)/np.sum(np.exp(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SGD(h, fJ, fdJ, theta, X, y, \n",
    "        alpha=0.001, maxEpochs=1, batchSize=100, adaGrad=True, shuffle=False):\n",
    "    m, n = X.shape\n",
    "    start, end = 0, batchSize\n",
    "    eps = 10 ** -7\n",
    "    historic_grads = np.matrix(np.zeros(n)).reshape(n, 1)\n",
    "    maxSteps = (m * float(maxEpochs)) / batchSize\n",
    "   \n",
    "    count = 0\n",
    "    for i in range(int(maxSteps)):\n",
    "        if shuffle and (i * batchSize) % m == 0:\n",
    "            indexes = [j for j in range(len(X))]\n",
    "            np.random.shuffle(indexes)\n",
    "            X = X[indexes]\n",
    "            y = y[indexes]\n",
    "        \n",
    "        XBatch, yBatch =  X[start:end,:], y[start:end,:]\n",
    "        grad = fdJ(h, theta, XBatch, yBatch)\n",
    "\n",
    "        if adaGrad:\n",
    "            historic_grads += np.multiply(grad, grad)\n",
    "            adjusted_grad = np.multiply(alpha / (np.sqrt(historic_grads) + eps), grad)\n",
    "        else:\n",
    "            adjusted_grad = alpha * grad\n",
    "\n",
    "        theta = theta - adjusted_grad\n",
    "        \n",
    "        if start + batchSize < m:\n",
    "            start += batchSize\n",
    "        else:\n",
    "            start = 0\n",
    "        end = min(start + batchSize, m)\n",
    "        \n",
    "    return theta\n",
    "\n",
    "def classify(thetas, X, debug=False):\n",
    "    regs = np.array([(X*theta).item() for theta in thetas])\n",
    "    if debug:\n",
    "        print(\"regs  =\", regs)\n",
    "    probs = softmax(regs)\n",
    "    if debug:\n",
    "        print(\"probs =\", np.around(probs, decimals=3))\n",
    "    return np.argmax(probs), probs\n",
    "\n",
    "def trainMaxEnt(X, Y, alpha = 1.0, maxEpochs = 2, batchSize = 50, adaGrad = True, shuffle = False):\n",
    "    n = X.shape[1]\n",
    "    thetas = []\n",
    "    for c in range(Y.shape[1]):\n",
    "        YBi = Y[:,c]\n",
    "        theta = np.matrix(np.random.random(n)).reshape(n,1)\n",
    "        thetaBest = SGD(h, J, dJ, theta, X, YBi, alpha, maxEpochs, batchSize, adaGrad, shuffle)\n",
    "        thetas.append(thetaBest)\n",
    "    return thetas\n",
    "\n",
    "def calculateAcc(thetas, X_test, y_test, debug=True):\n",
    "    acc = 0.0\n",
    "    for i in range(len(y_test)):\n",
    "        cls, probs = classify(thetas, X_test[i])\n",
    "        correctCls = int(y_test[i].item())\n",
    "        if i < 6 and debug:\n",
    "            print(correctCls, \"  <=>\", cls, \" -- \", cls == correctCls, np.round(probs, 4).tolist())\n",
    "        acc += correctCls == cls\n",
    "    return acc\n",
    "\n",
    "def calculateAccReturnClasses(thetas, X_test, y_test, debug=True):\n",
    "    acc = 0.0\n",
    "    predictions = []\n",
    "    for i in range(len(y_test)):\n",
    "        cls, probs = classify(thetas, X_test[i])\n",
    "        correctCls = int(y_test[i].item())\n",
    "        if i < 6 and debug:\n",
    "            print(correctCls, \"  <=>\", cls, \" -- \", cls == correctCls, np.round(probs, 4).tolist())\n",
    "        acc += correctCls == cls\n",
    "        predictions.append(cls)\n",
    "    return acc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_norm = normalize(X)\n",
    "X_test_norm = normalize(X_test)\n",
    "ind_y = indicatorMatrix(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7   <=> 7  --  True [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "2   <=> 2  --  True [0.0, 0.0, 0.9875, 0.0, 0.0, 0.0006, 0.0119, 0.0, 0.0, 0.0]\n",
      "1   <=> 1  --  True [0.0, 0.9999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0   <=> 0  --  True [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "4   <=> 4  --  True [0.0, 0.0, 0.0001, 0.0, 0.9983, 0.0, 0.0001, 0.0, 0.0, 0.0015]\n",
      "1   <=> 1  --  True [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Accuracy with Ada = 0.890600\n",
      "\n",
      "7   <=> 7  --  True [0.0, 0.0, 0.0001, 0.0, 0.0, 0.0, 0.0, 0.9999, 0.0, 0.0]\n",
      "2   <=> 2  --  True [0.0012, 0.0, 0.9745, 0.0003, 0.0, 0.0064, 0.017, 0.0, 0.0005, 0.0]\n",
      "1   <=> 1  --  True [0.0, 0.9981, 0.0002, 0.0003, 0.0, 0.0004, 0.0003, 0.0003, 0.0003, 0.0002]\n",
      "0   <=> 0  --  True [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "4   <=> 4  --  True [0.0, 0.0, 0.0002, 0.0, 0.99, 0.0, 0.0003, 0.0007, 0.0006, 0.0082]\n",
      "1   <=> 1  --  True [0.0, 0.9978, 0.0001, 0.0005, 0.0, 0.0, 0.0, 0.0006, 0.0006, 0.0002]\n",
      "Accuracy without Ada = 0.900700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gd_ada = trainMaxEnt(X_norm, ind_y, alpha = 1.0, maxEpochs = 2, batchSize = 50, adaGrad = True)\n",
    "print(\"Accuracy with Ada = %f\\n\" % (calculateAcc(gd_ada, X_test_norm, y_test)/len(X_test_norm)))\n",
    "\n",
    "gd_without_ada = trainMaxEnt(X_norm, ind_y, alpha = 0.1, maxEpochs = 2, batchSize = 50, adaGrad = False)\n",
    "print(\"Accuracy without Ada = %f\\n\" % (calculateAcc(gd_without_ada, X_test_norm, y_test)/len(X_test_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udało się uzyskać lepszy wynik, jednak wymagało to wypróbowania kilku możliwości parametru alpha i czasu na przetrenowanie danych po raz kolejny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy model 0 = 0.910800\n",
      "Accuracy model 1 = 0.907600\n",
      "Accuracy model 2 = 0.907300\n",
      "Accuracy model 3 = 0.907600\n",
      "Accuracy model 4 = 0.910000\n",
      "Accuracy model 5 = 0.909300\n",
      "Accuracy model 6 = 0.908000\n",
      "Accuracy model 7 = 0.907600\n",
      "Accuracy model 8 = 0.908500\n",
      "Accuracy model 9 = 0.909700\n",
      "\n",
      "Total accuracy = 0.911100\n"
     ]
    }
   ],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "models_predictions = []\n",
    "for i in range(10):\n",
    "    indexes = [j for j in range(len(X_norm))]\n",
    "    np.random.shuffle(indexes)\n",
    "    X_norm = X_norm[indexes]\n",
    "    ind_y = ind_y[indexes]\n",
    "    \n",
    "    gd = trainMaxEnt(X_norm, ind_y, alpha=0.1, maxEpochs=3, batchSize=50, adaGrad=True, shuffle=True)\n",
    "    acc, predicts = calculateAccReturnClasses(gd, X_test_norm, y_test, debug=False)\n",
    "    models_predictions.append(predicts)\n",
    "    print(\"Accuracy model %d = %f\" % (i, acc/len(X_test_norm)))\n",
    "    \n",
    "acc = 0.0\n",
    "for i in range(len(y_test)):\n",
    "    predictions_row = []\n",
    "    for j in range(len(models_predictions)):\n",
    "        predictions_row.append(models_predictions[j][i])\n",
    "    freq = most_common(predictions_row)\n",
    "    correctCls = int(y_test[i].item())\n",
    "    acc += correctCls == freq\n",
    "    \n",
    "print(\"\\nTotal accuracy = %f\" % (acc/len(X_test_norm)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatecznie udało się uzyskać accuracy wyższe niż w przypadku każdego z pojedynczych modeli, użycie Ensemble polepszyło wynik."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
