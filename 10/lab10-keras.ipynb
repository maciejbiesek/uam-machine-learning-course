{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wielowarstwowe sieci neuronowe\n",
    "\n",
    "[Keras](https://keras.io/) to zaawansowany pakiet do tworzenia sieci neuronowych w języku Python.\n",
    "Na komputerach wydziałowych powinien być zainstalowany.\n",
    "Na własnych komputerach pod Linuxem można go zainstalować poleceniem: `sudo pip install keras`.\n",
    "\n",
    "**Uwaga:** pierwsze uruchomienie zazwyczaj trwa jakiś czas, ponieważ pod spodem model kompiluje się jako aplikacja w C++. \n",
    "\n",
    "#### 1. Iris dataset\n",
    "\n",
    "Korzystając z [oficjalnej dokumentacji](http://keras.io) oraz materiałów szkoleniowych znalezionych w internecie (np. [machinelearningmastery.com](http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/)), zbuduj (co najmniej) dwuwarstwową sieć neuronową do klasyfkacji _Iris dataset_. Opisz stworzony model: architekturę sieci, jej rozmiar, zastosowane funkcje aktywacji, funkcję kosztu, wersję GD, metodę regularyzacji. Podaj wynik ewaluacji na zbiorze testowym.\n",
    "\n",
    "#### 2. MNIST\n",
    "\n",
    "Uruchom przykład `mnist_mlp.py` z [katalogu oficjalnych przykładów]( https://github.com/fchollet/keras/tree/master/examples) (warto ew. zmienić liczbę epok do 5). Posiłkując się dokumentacją, przeanalizuj kod i opisz:\n",
    "\n",
    "* Do jakiej postaci sprowadzane są dane `Y_train` i `Y_test`?\n",
    "* Przedstaw wzór matematyczny na zastosowaną funkcję błędu.\n",
    "* Jaka jest architektura sieci neuronowej? Ile ma warstw, jakie są rozmiary macierzy warstw? Czy można uzyskać dostęp do tych wag?\n",
    "* Jakie funkcje aktywacji użyto? Podaj ich wzory.\n",
    "* Czym jest `Dropout`? Czemu służy? Jakie znaczenie ma parametr?\n",
    "\n",
    "Zmodyfikuj model z przykładu `mnist_mlp.py` i wykonaj:\n",
    "\n",
    "* Usuń warstwy `Dropout`, jaki jest efekt?\n",
    "* Stwórz 6-cio warstwowy model o rozmiarach warstw 2500, 2000, 1500, 1000, 500 oraz 10 bez `Dropout`, użyj wszędzie funkcji aktywacji `tanh` z wyjątkiem ostatniej warstwy, gdzie należy użyć `softmax`. Trenuj model przez 10 epok.\n",
    "* Dodaj warstwy `Dropout`, porównaj jakość po 10 epokach, krótko opisz wnioski.\n",
    "* Zamiast `RMSprop` użyj algorytm `Adagrad`, porównaj jakość, krótko opisz wnioski. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 3)\n",
    "y_test = np_utils.to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/250\n",
      "120/120 [==============================] - 1s - loss: 1.5091 - acc: 0.3417 - val_loss: 1.4650 - val_acc: 0.3000\n",
      "Epoch 2/250\n",
      "120/120 [==============================] - 0s - loss: 1.4829 - acc: 0.3417 - val_loss: 1.4419 - val_acc: 0.3000\n",
      "Epoch 3/250\n",
      "120/120 [==============================] - 0s - loss: 1.4577 - acc: 0.3417 - val_loss: 1.4199 - val_acc: 0.3000\n",
      "Epoch 4/250\n",
      "120/120 [==============================] - 0s - loss: 1.4334 - acc: 0.3417 - val_loss: 1.3995 - val_acc: 0.3000\n",
      "Epoch 5/250\n",
      "120/120 [==============================] - 0s - loss: 1.4111 - acc: 0.3417 - val_loss: 1.3807 - val_acc: 0.3000\n",
      "Epoch 6/250\n",
      "120/120 [==============================] - 0s - loss: 1.3904 - acc: 0.3417 - val_loss: 1.3631 - val_acc: 0.3000\n",
      "Epoch 7/250\n",
      "120/120 [==============================] - 0s - loss: 1.3709 - acc: 0.3417 - val_loss: 1.3462 - val_acc: 0.3000\n",
      "Epoch 8/250\n",
      "120/120 [==============================] - 0s - loss: 1.3519 - acc: 0.3417 - val_loss: 1.3284 - val_acc: 0.3000\n",
      "Epoch 9/250\n",
      "120/120 [==============================] - 0s - loss: 1.3315 - acc: 0.3417 - val_loss: 1.3088 - val_acc: 0.3000\n",
      "Epoch 10/250\n",
      "120/120 [==============================] - 0s - loss: 1.3093 - acc: 0.3417 - val_loss: 1.2863 - val_acc: 0.3000\n",
      "Epoch 11/250\n",
      "120/120 [==============================] - 0s - loss: 1.2837 - acc: 0.3417 - val_loss: 1.2610 - val_acc: 0.3000\n",
      "Epoch 12/250\n",
      "120/120 [==============================] - 0s - loss: 1.2553 - acc: 0.3417 - val_loss: 1.2342 - val_acc: 0.3000\n",
      "Epoch 13/250\n",
      "120/120 [==============================] - 0s - loss: 1.2274 - acc: 0.3333 - val_loss: 1.2079 - val_acc: 0.2667\n",
      "Epoch 14/250\n",
      "120/120 [==============================] - 0s - loss: 1.2019 - acc: 0.3083 - val_loss: 1.1855 - val_acc: 0.2667\n",
      "Epoch 15/250\n",
      "120/120 [==============================] - 0s - loss: 1.1820 - acc: 0.2750 - val_loss: 1.1680 - val_acc: 0.2333\n",
      "Epoch 16/250\n",
      "120/120 [==============================] - 0s - loss: 1.1668 - acc: 0.2417 - val_loss: 1.1546 - val_acc: 0.2000\n",
      "Epoch 17/250\n",
      "120/120 [==============================] - 0s - loss: 1.1547 - acc: 0.2083 - val_loss: 1.1444 - val_acc: 0.2000\n",
      "Epoch 18/250\n",
      "120/120 [==============================] - 0s - loss: 1.1454 - acc: 0.2000 - val_loss: 1.1356 - val_acc: 0.2000\n",
      "Epoch 19/250\n",
      "120/120 [==============================] - 0s - loss: 1.1373 - acc: 0.2083 - val_loss: 1.1283 - val_acc: 0.2333\n",
      "Epoch 20/250\n",
      "120/120 [==============================] - 0s - loss: 1.1303 - acc: 0.2417 - val_loss: 1.1218 - val_acc: 0.2333\n",
      "Epoch 21/250\n",
      "120/120 [==============================] - 0s - loss: 1.1245 - acc: 0.2417 - val_loss: 1.1157 - val_acc: 0.2333\n",
      "Epoch 22/250\n",
      "120/120 [==============================] - 0s - loss: 1.1192 - acc: 0.2500 - val_loss: 1.1103 - val_acc: 0.2333\n",
      "Epoch 23/250\n",
      "120/120 [==============================] - 0s - loss: 1.1146 - acc: 0.3083 - val_loss: 1.1059 - val_acc: 0.3000\n",
      "Epoch 24/250\n",
      "120/120 [==============================] - 0s - loss: 1.1106 - acc: 0.3667 - val_loss: 1.1020 - val_acc: 0.3667\n",
      "Epoch 25/250\n",
      "120/120 [==============================] - 0s - loss: 1.1070 - acc: 0.4250 - val_loss: 1.0985 - val_acc: 0.4667\n",
      "Epoch 26/250\n",
      "120/120 [==============================] - 0s - loss: 1.1042 - acc: 0.4333 - val_loss: 1.0952 - val_acc: 0.5000\n",
      "Epoch 27/250\n",
      "120/120 [==============================] - 0s - loss: 1.1015 - acc: 0.4250 - val_loss: 1.0921 - val_acc: 0.4667\n",
      "Epoch 28/250\n",
      "120/120 [==============================] - 0s - loss: 1.0986 - acc: 0.4333 - val_loss: 1.0895 - val_acc: 0.5000\n",
      "Epoch 29/250\n",
      "120/120 [==============================] - 0s - loss: 1.0961 - acc: 0.4333 - val_loss: 1.0874 - val_acc: 0.4667\n",
      "Epoch 30/250\n",
      "120/120 [==============================] - 0s - loss: 1.0939 - acc: 0.4333 - val_loss: 1.0850 - val_acc: 0.5000\n",
      "Epoch 31/250\n",
      "120/120 [==============================] - 0s - loss: 1.0915 - acc: 0.4417 - val_loss: 1.0827 - val_acc: 0.5000\n",
      "Epoch 32/250\n",
      "120/120 [==============================] - 0s - loss: 1.0895 - acc: 0.4417 - val_loss: 1.0801 - val_acc: 0.5000\n",
      "Epoch 33/250\n",
      "120/120 [==============================] - 0s - loss: 1.0873 - acc: 0.4333 - val_loss: 1.0777 - val_acc: 0.5000\n",
      "Epoch 34/250\n",
      "120/120 [==============================] - 0s - loss: 1.0851 - acc: 0.4417 - val_loss: 1.0756 - val_acc: 0.5000\n",
      "Epoch 35/250\n",
      "120/120 [==============================] - 0s - loss: 1.0831 - acc: 0.4417 - val_loss: 1.0737 - val_acc: 0.5000\n",
      "Epoch 36/250\n",
      "120/120 [==============================] - 0s - loss: 1.0808 - acc: 0.4417 - val_loss: 1.0711 - val_acc: 0.5000\n",
      "Epoch 37/250\n",
      "120/120 [==============================] - 0s - loss: 1.0788 - acc: 0.4500 - val_loss: 1.0686 - val_acc: 0.5000\n",
      "Epoch 38/250\n",
      "120/120 [==============================] - 0s - loss: 1.0767 - acc: 0.4417 - val_loss: 1.0668 - val_acc: 0.5000\n",
      "Epoch 39/250\n",
      "120/120 [==============================] - 0s - loss: 1.0745 - acc: 0.4500 - val_loss: 1.0648 - val_acc: 0.5333\n",
      "Epoch 40/250\n",
      "120/120 [==============================] - 0s - loss: 1.0720 - acc: 0.4500 - val_loss: 1.0620 - val_acc: 0.5000\n",
      "Epoch 41/250\n",
      "120/120 [==============================] - 0s - loss: 1.0696 - acc: 0.5083 - val_loss: 1.0593 - val_acc: 0.5667\n",
      "Epoch 42/250\n",
      "120/120 [==============================] - 0s - loss: 1.0673 - acc: 0.5333 - val_loss: 1.0570 - val_acc: 0.6333\n",
      "Epoch 43/250\n",
      "120/120 [==============================] - 0s - loss: 1.0648 - acc: 0.5833 - val_loss: 1.0545 - val_acc: 0.6667\n",
      "Epoch 44/250\n",
      "120/120 [==============================] - 0s - loss: 1.0623 - acc: 0.6833 - val_loss: 1.0520 - val_acc: 0.8000\n",
      "Epoch 45/250\n",
      "120/120 [==============================] - 0s - loss: 1.0599 - acc: 0.7250 - val_loss: 1.0495 - val_acc: 0.8333\n",
      "Epoch 46/250\n",
      "120/120 [==============================] - 0s - loss: 1.0573 - acc: 0.8000 - val_loss: 1.0462 - val_acc: 0.8333\n",
      "Epoch 47/250\n",
      "120/120 [==============================] - 0s - loss: 1.0547 - acc: 0.8333 - val_loss: 1.0434 - val_acc: 0.8667\n",
      "Epoch 48/250\n",
      "120/120 [==============================] - 0s - loss: 1.0519 - acc: 0.8417 - val_loss: 1.0405 - val_acc: 0.9000\n",
      "Epoch 49/250\n",
      "120/120 [==============================] - 0s - loss: 1.0491 - acc: 0.8500 - val_loss: 1.0376 - val_acc: 0.9000\n",
      "Epoch 50/250\n",
      "120/120 [==============================] - 0s - loss: 1.0463 - acc: 0.8500 - val_loss: 1.0353 - val_acc: 0.9333\n",
      "Epoch 51/250\n",
      "120/120 [==============================] - 0s - loss: 1.0435 - acc: 0.8500 - val_loss: 1.0317 - val_acc: 0.9333\n",
      "Epoch 52/250\n",
      "120/120 [==============================] - 0s - loss: 1.0403 - acc: 0.8500 - val_loss: 1.0288 - val_acc: 0.9333\n",
      "Epoch 53/250\n",
      "120/120 [==============================] - 0s - loss: 1.0373 - acc: 0.8500 - val_loss: 1.0255 - val_acc: 0.9333\n",
      "Epoch 54/250\n",
      "120/120 [==============================] - 0s - loss: 1.0342 - acc: 0.8500 - val_loss: 1.0223 - val_acc: 0.9333\n",
      "Epoch 55/250\n",
      "120/120 [==============================] - 0s - loss: 1.0310 - acc: 0.8583 - val_loss: 1.0187 - val_acc: 0.9667\n",
      "Epoch 56/250\n",
      "120/120 [==============================] - 0s - loss: 1.0280 - acc: 0.8583 - val_loss: 1.0152 - val_acc: 0.9667\n",
      "Epoch 57/250\n",
      "120/120 [==============================] - 0s - loss: 1.0247 - acc: 0.8583 - val_loss: 1.0121 - val_acc: 0.9667\n",
      "Epoch 58/250\n",
      "120/120 [==============================] - 0s - loss: 1.0214 - acc: 0.8583 - val_loss: 1.0085 - val_acc: 0.9667\n",
      "Epoch 59/250\n",
      "120/120 [==============================] - 0s - loss: 1.0180 - acc: 0.8583 - val_loss: 1.0053 - val_acc: 0.9667\n",
      "Epoch 60/250\n",
      "120/120 [==============================] - 0s - loss: 1.0149 - acc: 0.8583 - val_loss: 1.0024 - val_acc: 0.9667\n",
      "Epoch 61/250\n",
      "120/120 [==============================] - 0s - loss: 1.0115 - acc: 0.8583 - val_loss: 0.9978 - val_acc: 0.9667\n",
      "Epoch 62/250\n",
      "120/120 [==============================] - 0s - loss: 1.0078 - acc: 0.8583 - val_loss: 0.9940 - val_acc: 0.9667\n",
      "Epoch 63/250\n",
      "120/120 [==============================] - 0s - loss: 1.0044 - acc: 0.8583 - val_loss: 0.9907 - val_acc: 0.9667\n",
      "Epoch 64/250\n",
      "120/120 [==============================] - 0s - loss: 1.0008 - acc: 0.8583 - val_loss: 0.9870 - val_acc: 0.9667\n",
      "Epoch 65/250\n",
      "120/120 [==============================] - 0s - loss: 0.9972 - acc: 0.8583 - val_loss: 0.9837 - val_acc: 0.9667\n",
      "Epoch 66/250\n",
      "120/120 [==============================] - 0s - loss: 0.9939 - acc: 0.8667 - val_loss: 0.9803 - val_acc: 0.9667\n",
      "Epoch 67/250\n",
      "120/120 [==============================] - 0s - loss: 0.9901 - acc: 0.8667 - val_loss: 0.9761 - val_acc: 0.9667\n",
      "Epoch 68/250\n",
      "120/120 [==============================] - 0s - loss: 0.9864 - acc: 0.8667 - val_loss: 0.9720 - val_acc: 0.9333\n",
      "Epoch 69/250\n",
      "120/120 [==============================] - 0s - loss: 0.9828 - acc: 0.8667 - val_loss: 0.9681 - val_acc: 0.9333\n",
      "Epoch 70/250\n",
      "120/120 [==============================] - 0s - loss: 0.9793 - acc: 0.8667 - val_loss: 0.9645 - val_acc: 0.9333\n",
      "Epoch 71/250\n",
      "120/120 [==============================] - 0s - loss: 0.9756 - acc: 0.8667 - val_loss: 0.9608 - val_acc: 0.9333\n",
      "Epoch 72/250\n",
      "120/120 [==============================] - 0s - loss: 0.9720 - acc: 0.8667 - val_loss: 0.9569 - val_acc: 0.9333\n",
      "Epoch 73/250\n",
      "120/120 [==============================] - 0s - loss: 0.9683 - acc: 0.8750 - val_loss: 0.9534 - val_acc: 0.9333\n",
      "Epoch 74/250\n",
      "120/120 [==============================] - 0s - loss: 0.9647 - acc: 0.8750 - val_loss: 0.9491 - val_acc: 0.9333\n",
      "Epoch 75/250\n",
      "120/120 [==============================] - 0s - loss: 0.9612 - acc: 0.8833 - val_loss: 0.9463 - val_acc: 0.9333\n",
      "Epoch 76/250\n",
      "120/120 [==============================] - 0s - loss: 0.9573 - acc: 0.9000 - val_loss: 0.9420 - val_acc: 0.9333\n",
      "Epoch 77/250\n",
      "120/120 [==============================] - 0s - loss: 0.9537 - acc: 0.8917 - val_loss: 0.9377 - val_acc: 0.9333\n",
      "Epoch 78/250\n",
      "120/120 [==============================] - 0s - loss: 0.9499 - acc: 0.8750 - val_loss: 0.9340 - val_acc: 0.9333\n",
      "Epoch 79/250\n",
      "120/120 [==============================] - 0s - loss: 0.9463 - acc: 0.8833 - val_loss: 0.9306 - val_acc: 0.9333\n",
      "Epoch 80/250\n",
      "120/120 [==============================] - 0s - loss: 0.9426 - acc: 0.9000 - val_loss: 0.9266 - val_acc: 0.9333\n",
      "Epoch 81/250\n",
      "120/120 [==============================] - 0s - loss: 0.9389 - acc: 0.9000 - val_loss: 0.9233 - val_acc: 0.9333\n",
      "Epoch 82/250\n",
      "120/120 [==============================] - 0s - loss: 0.9352 - acc: 0.9000 - val_loss: 0.9193 - val_acc: 0.9333\n",
      "Epoch 83/250\n",
      "120/120 [==============================] - 0s - loss: 0.9315 - acc: 0.9000 - val_loss: 0.9155 - val_acc: 0.9333\n",
      "Epoch 84/250\n",
      "120/120 [==============================] - 0s - loss: 0.9281 - acc: 0.9000 - val_loss: 0.9120 - val_acc: 0.9333\n",
      "Epoch 85/250\n",
      "120/120 [==============================] - 0s - loss: 0.9244 - acc: 0.8917 - val_loss: 0.9074 - val_acc: 0.9000\n",
      "Epoch 86/250\n",
      "120/120 [==============================] - 0s - loss: 0.9207 - acc: 0.8917 - val_loss: 0.9037 - val_acc: 0.9000\n",
      "Epoch 87/250\n",
      "120/120 [==============================] - 0s - loss: 0.9172 - acc: 0.9083 - val_loss: 0.9007 - val_acc: 0.9333\n",
      "Epoch 88/250\n",
      "120/120 [==============================] - 0s - loss: 0.9134 - acc: 0.9167 - val_loss: 0.8968 - val_acc: 0.9333\n",
      "Epoch 89/250\n",
      "120/120 [==============================] - 0s - loss: 0.9098 - acc: 0.9083 - val_loss: 0.8932 - val_acc: 0.9333\n",
      "Epoch 90/250\n",
      "120/120 [==============================] - 0s - loss: 0.9070 - acc: 0.9250 - val_loss: 0.8907 - val_acc: 0.9333\n",
      "Epoch 91/250\n",
      "120/120 [==============================] - 0s - loss: 0.9026 - acc: 0.9167 - val_loss: 0.8860 - val_acc: 0.9333\n",
      "Epoch 92/250\n",
      "120/120 [==============================] - 0s - loss: 0.8990 - acc: 0.9083 - val_loss: 0.8816 - val_acc: 0.9000\n",
      "Epoch 93/250\n",
      "120/120 [==============================] - 0s - loss: 0.8955 - acc: 0.9000 - val_loss: 0.8781 - val_acc: 0.9000\n",
      "Epoch 94/250\n",
      "120/120 [==============================] - 0s - loss: 0.8921 - acc: 0.9000 - val_loss: 0.8747 - val_acc: 0.9000\n",
      "Epoch 95/250\n",
      "120/120 [==============================] - 0s - loss: 0.8884 - acc: 0.9000 - val_loss: 0.8711 - val_acc: 0.9000\n",
      "Epoch 96/250\n",
      "120/120 [==============================] - 0s - loss: 0.8849 - acc: 0.9167 - val_loss: 0.8681 - val_acc: 0.9333\n",
      "Epoch 97/250\n",
      "120/120 [==============================] - 0s - loss: 0.8814 - acc: 0.9167 - val_loss: 0.8643 - val_acc: 0.9333\n",
      "Epoch 98/250\n",
      "120/120 [==============================] - 0s - loss: 0.8780 - acc: 0.9083 - val_loss: 0.8605 - val_acc: 0.9000\n",
      "Epoch 99/250\n",
      "120/120 [==============================] - 0s - loss: 0.8744 - acc: 0.9083 - val_loss: 0.8573 - val_acc: 0.9333\n",
      "Epoch 100/250\n",
      "120/120 [==============================] - 0s - loss: 0.8710 - acc: 0.9333 - val_loss: 0.8542 - val_acc: 0.9333\n",
      "Epoch 101/250\n",
      "120/120 [==============================] - 0s - loss: 0.8673 - acc: 0.9333 - val_loss: 0.8503 - val_acc: 0.9333\n",
      "Epoch 102/250\n",
      "120/120 [==============================] - 0s - loss: 0.8642 - acc: 0.9167 - val_loss: 0.8464 - val_acc: 0.9000\n",
      "Epoch 103/250\n",
      "120/120 [==============================] - 0s - loss: 0.8607 - acc: 0.9083 - val_loss: 0.8426 - val_acc: 0.9000\n",
      "Epoch 104/250\n",
      "120/120 [==============================] - 0s - loss: 0.8572 - acc: 0.9083 - val_loss: 0.8391 - val_acc: 0.9000\n",
      "Epoch 105/250\n",
      "120/120 [==============================] - 0s - loss: 0.8536 - acc: 0.9083 - val_loss: 0.8359 - val_acc: 0.9000\n",
      "Epoch 106/250\n",
      "120/120 [==============================] - 0s - loss: 0.8504 - acc: 0.9250 - val_loss: 0.8338 - val_acc: 0.9333\n",
      "Epoch 107/250\n",
      "120/120 [==============================] - 0s - loss: 0.8472 - acc: 0.9417 - val_loss: 0.8300 - val_acc: 0.9333\n",
      "Epoch 108/250\n",
      "120/120 [==============================] - 0s - loss: 0.8437 - acc: 0.9083 - val_loss: 0.8253 - val_acc: 0.9000\n",
      "Epoch 109/250\n",
      "120/120 [==============================] - 0s - loss: 0.8402 - acc: 0.9083 - val_loss: 0.8223 - val_acc: 0.9000\n",
      "Epoch 110/250\n",
      "120/120 [==============================] - 0s - loss: 0.8369 - acc: 0.9083 - val_loss: 0.8187 - val_acc: 0.9000\n",
      "Epoch 111/250\n",
      "120/120 [==============================] - 0s - loss: 0.8333 - acc: 0.9250 - val_loss: 0.8161 - val_acc: 0.9333\n",
      "Epoch 112/250\n",
      "120/120 [==============================] - 0s - loss: 0.8300 - acc: 0.9417 - val_loss: 0.8128 - val_acc: 0.9333\n",
      "Epoch 113/250\n",
      "120/120 [==============================] - 0s - loss: 0.8266 - acc: 0.9417 - val_loss: 0.8090 - val_acc: 0.9000\n",
      "Epoch 114/250\n",
      "120/120 [==============================] - 0s - loss: 0.8233 - acc: 0.9333 - val_loss: 0.8053 - val_acc: 0.9000\n",
      "Epoch 115/250\n",
      "120/120 [==============================] - 0s - loss: 0.8204 - acc: 0.9167 - val_loss: 0.8019 - val_acc: 0.9000\n",
      "Epoch 116/250\n",
      "120/120 [==============================] - 0s - loss: 0.8173 - acc: 0.9333 - val_loss: 0.8001 - val_acc: 0.9333\n",
      "Epoch 117/250\n",
      "120/120 [==============================] - 0s - loss: 0.8137 - acc: 0.9250 - val_loss: 0.7952 - val_acc: 0.9000\n",
      "Epoch 118/250\n",
      "120/120 [==============================] - 0s - loss: 0.8100 - acc: 0.9167 - val_loss: 0.7920 - val_acc: 0.9000\n",
      "Epoch 119/250\n",
      "120/120 [==============================] - 0s - loss: 0.8069 - acc: 0.9250 - val_loss: 0.7896 - val_acc: 0.9333\n",
      "Epoch 120/250\n",
      "120/120 [==============================] - 0s - loss: 0.8035 - acc: 0.9333 - val_loss: 0.7861 - val_acc: 0.9000\n",
      "Epoch 121/250\n",
      "120/120 [==============================] - 0s - loss: 0.8005 - acc: 0.9417 - val_loss: 0.7831 - val_acc: 0.9333\n",
      "Epoch 122/250\n",
      "120/120 [==============================] - 0s - loss: 0.7974 - acc: 0.9333 - val_loss: 0.7786 - val_acc: 0.9000\n",
      "Epoch 123/250\n",
      "120/120 [==============================] - 0s - loss: 0.7939 - acc: 0.9250 - val_loss: 0.7761 - val_acc: 0.9000\n",
      "Epoch 124/250\n",
      "120/120 [==============================] - 0s - loss: 0.7905 - acc: 0.9417 - val_loss: 0.7734 - val_acc: 0.9000\n",
      "Epoch 125/250\n",
      "120/120 [==============================] - 0s - loss: 0.7873 - acc: 0.9500 - val_loss: 0.7696 - val_acc: 0.9000\n",
      "Epoch 126/250\n",
      "120/120 [==============================] - 0s - loss: 0.7842 - acc: 0.9333 - val_loss: 0.7661 - val_acc: 0.9000\n",
      "Epoch 127/250\n",
      "120/120 [==============================] - 0s - loss: 0.7811 - acc: 0.9583 - val_loss: 0.7644 - val_acc: 0.9333\n",
      "Epoch 128/250\n",
      "120/120 [==============================] - 0s - loss: 0.7776 - acc: 0.9583 - val_loss: 0.7607 - val_acc: 0.9000\n",
      "Epoch 129/250\n",
      "120/120 [==============================] - 0s - loss: 0.7746 - acc: 0.9417 - val_loss: 0.7573 - val_acc: 0.9000\n",
      "Epoch 130/250\n",
      "120/120 [==============================] - 0s - loss: 0.7715 - acc: 0.9250 - val_loss: 0.7532 - val_acc: 0.9000\n",
      "Epoch 131/250\n",
      "120/120 [==============================] - 0s - loss: 0.7681 - acc: 0.9417 - val_loss: 0.7509 - val_acc: 0.9000\n",
      "Epoch 132/250\n",
      "120/120 [==============================] - 0s - loss: 0.7647 - acc: 0.9500 - val_loss: 0.7476 - val_acc: 0.9000\n",
      "Epoch 133/250\n",
      "120/120 [==============================] - 0s - loss: 0.7616 - acc: 0.9500 - val_loss: 0.7451 - val_acc: 0.9000\n",
      "Epoch 134/250\n",
      "120/120 [==============================] - 0s - loss: 0.7583 - acc: 0.9583 - val_loss: 0.7415 - val_acc: 0.9000\n",
      "Epoch 135/250\n",
      "120/120 [==============================] - 0s - loss: 0.7555 - acc: 0.9500 - val_loss: 0.7380 - val_acc: 0.9000\n",
      "Epoch 136/250\n",
      "120/120 [==============================] - 0s - loss: 0.7520 - acc: 0.9500 - val_loss: 0.7353 - val_acc: 0.9000\n",
      "Epoch 137/250\n",
      "120/120 [==============================] - 0s - loss: 0.7490 - acc: 0.9500 - val_loss: 0.7332 - val_acc: 0.9333\n",
      "Epoch 138/250\n",
      "120/120 [==============================] - 0s - loss: 0.7459 - acc: 0.9583 - val_loss: 0.7292 - val_acc: 0.9000\n",
      "Epoch 139/250\n",
      "120/120 [==============================] - 0s - loss: 0.7425 - acc: 0.9583 - val_loss: 0.7260 - val_acc: 0.9000\n",
      "Epoch 140/250\n",
      "120/120 [==============================] - 0s - loss: 0.7398 - acc: 0.9500 - val_loss: 0.7225 - val_acc: 0.9000\n",
      "Epoch 141/250\n",
      "120/120 [==============================] - 0s - loss: 0.7366 - acc: 0.9667 - val_loss: 0.7209 - val_acc: 0.9333\n",
      "Epoch 142/250\n",
      "120/120 [==============================] - 0s - loss: 0.7332 - acc: 0.9667 - val_loss: 0.7172 - val_acc: 0.9000\n",
      "Epoch 143/250\n",
      "120/120 [==============================] - 0s - loss: 0.7300 - acc: 0.9583 - val_loss: 0.7139 - val_acc: 0.9000\n",
      "Epoch 144/250\n",
      "120/120 [==============================] - 0s - loss: 0.7270 - acc: 0.9500 - val_loss: 0.7108 - val_acc: 0.9000\n",
      "Epoch 145/250\n",
      "120/120 [==============================] - 0s - loss: 0.7239 - acc: 0.9583 - val_loss: 0.7081 - val_acc: 0.9000\n",
      "Epoch 146/250\n",
      "120/120 [==============================] - 0s - loss: 0.7207 - acc: 0.9667 - val_loss: 0.7056 - val_acc: 0.9000\n",
      "Epoch 147/250\n",
      "120/120 [==============================] - 0s - loss: 0.7178 - acc: 0.9583 - val_loss: 0.7020 - val_acc: 0.9000\n",
      "Epoch 148/250\n",
      "120/120 [==============================] - 0s - loss: 0.7147 - acc: 0.9583 - val_loss: 0.6990 - val_acc: 0.9000\n",
      "Epoch 149/250\n",
      "120/120 [==============================] - 0s - loss: 0.7113 - acc: 0.9667 - val_loss: 0.6964 - val_acc: 0.9000\n",
      "Epoch 150/250\n",
      "120/120 [==============================] - 0s - loss: 0.7090 - acc: 0.9667 - val_loss: 0.6946 - val_acc: 0.9333\n",
      "Epoch 151/250\n",
      "120/120 [==============================] - 0s - loss: 0.7054 - acc: 0.9667 - val_loss: 0.6902 - val_acc: 0.9000\n",
      "Epoch 152/250\n",
      "120/120 [==============================] - 0s - loss: 0.7022 - acc: 0.9667 - val_loss: 0.6870 - val_acc: 0.9000\n",
      "Epoch 153/250\n",
      "120/120 [==============================] - 0s - loss: 0.6992 - acc: 0.9667 - val_loss: 0.6842 - val_acc: 0.9000\n",
      "Epoch 154/250\n",
      "120/120 [==============================] - 0s - loss: 0.6969 - acc: 0.9667 - val_loss: 0.6823 - val_acc: 0.9333\n",
      "Epoch 155/250\n",
      "120/120 [==============================] - 0s - loss: 0.6932 - acc: 0.9667 - val_loss: 0.6782 - val_acc: 0.9000\n",
      "Epoch 156/250\n",
      "120/120 [==============================] - 0s - loss: 0.6902 - acc: 0.9583 - val_loss: 0.6755 - val_acc: 0.9000\n",
      "Epoch 157/250\n",
      "120/120 [==============================] - 0s - loss: 0.6873 - acc: 0.9667 - val_loss: 0.6732 - val_acc: 0.9000\n",
      "Epoch 158/250\n",
      "120/120 [==============================] - 0s - loss: 0.6842 - acc: 0.9667 - val_loss: 0.6706 - val_acc: 0.9000\n",
      "Epoch 159/250\n",
      "120/120 [==============================] - 0s - loss: 0.6813 - acc: 0.9667 - val_loss: 0.6668 - val_acc: 0.9000\n",
      "Epoch 160/250\n",
      "120/120 [==============================] - 0s - loss: 0.6795 - acc: 0.9667 - val_loss: 0.6653 - val_acc: 0.9333\n",
      "Epoch 161/250\n",
      "120/120 [==============================] - 0s - loss: 0.6752 - acc: 0.9667 - val_loss: 0.6612 - val_acc: 0.9000\n",
      "Epoch 162/250\n",
      "120/120 [==============================] - 0s - loss: 0.6724 - acc: 0.9667 - val_loss: 0.6584 - val_acc: 0.9000\n",
      "Epoch 163/250\n",
      "120/120 [==============================] - 0s - loss: 0.6694 - acc: 0.9667 - val_loss: 0.6560 - val_acc: 0.9000\n",
      "Epoch 164/250\n",
      "120/120 [==============================] - 0s - loss: 0.6664 - acc: 0.9667 - val_loss: 0.6531 - val_acc: 0.9000\n",
      "Epoch 165/250\n",
      "120/120 [==============================] - 0s - loss: 0.6642 - acc: 0.9667 - val_loss: 0.6514 - val_acc: 0.9333\n",
      "Epoch 166/250\n",
      "120/120 [==============================] - 0s - loss: 0.6606 - acc: 0.9750 - val_loss: 0.6474 - val_acc: 0.9000\n",
      "Epoch 167/250\n",
      "120/120 [==============================] - 0s - loss: 0.6578 - acc: 0.9667 - val_loss: 0.6447 - val_acc: 0.9000\n",
      "Epoch 168/250\n",
      "120/120 [==============================] - 0s - loss: 0.6548 - acc: 0.9750 - val_loss: 0.6419 - val_acc: 0.9000\n",
      "Epoch 169/250\n",
      "120/120 [==============================] - 0s - loss: 0.6519 - acc: 0.9750 - val_loss: 0.6397 - val_acc: 0.9000\n",
      "Epoch 170/250\n",
      "120/120 [==============================] - 0s - loss: 0.6489 - acc: 0.9750 - val_loss: 0.6369 - val_acc: 0.9000\n",
      "Epoch 171/250\n",
      "120/120 [==============================] - 0s - loss: 0.6463 - acc: 0.9750 - val_loss: 0.6338 - val_acc: 0.9000\n",
      "Epoch 172/250\n",
      "120/120 [==============================] - 0s - loss: 0.6440 - acc: 0.9750 - val_loss: 0.6323 - val_acc: 0.9333\n",
      "Epoch 173/250\n",
      "120/120 [==============================] - 0s - loss: 0.6404 - acc: 0.9750 - val_loss: 0.6284 - val_acc: 0.9000\n",
      "Epoch 174/250\n",
      "120/120 [==============================] - 0s - loss: 0.6377 - acc: 0.9750 - val_loss: 0.6258 - val_acc: 0.9000\n",
      "Epoch 175/250\n",
      "120/120 [==============================] - 0s - loss: 0.6349 - acc: 0.9750 - val_loss: 0.6231 - val_acc: 0.9000\n",
      "Epoch 176/250\n",
      "120/120 [==============================] - 0s - loss: 0.6323 - acc: 0.9750 - val_loss: 0.6204 - val_acc: 0.9000\n",
      "Epoch 177/250\n",
      "120/120 [==============================] - 0s - loss: 0.6293 - acc: 0.9750 - val_loss: 0.6184 - val_acc: 0.9333\n",
      "Epoch 178/250\n",
      "120/120 [==============================] - 0s - loss: 0.6267 - acc: 0.9833 - val_loss: 0.6158 - val_acc: 0.9333\n",
      "Epoch 179/250\n",
      "120/120 [==============================] - 0s - loss: 0.6238 - acc: 0.9833 - val_loss: 0.6130 - val_acc: 0.9333\n",
      "Epoch 180/250\n",
      "120/120 [==============================] - 0s - loss: 0.6210 - acc: 0.9750 - val_loss: 0.6098 - val_acc: 0.9000\n",
      "Epoch 181/250\n",
      "120/120 [==============================] - 0s - loss: 0.6187 - acc: 0.9750 - val_loss: 0.6073 - val_acc: 0.9000\n",
      "Epoch 182/250\n",
      "120/120 [==============================] - 0s - loss: 0.6159 - acc: 0.9833 - val_loss: 0.6057 - val_acc: 0.9333\n",
      "Epoch 183/250\n",
      "120/120 [==============================] - 0s - loss: 0.6132 - acc: 0.9833 - val_loss: 0.6024 - val_acc: 0.9000\n",
      "Epoch 184/250\n",
      "120/120 [==============================] - 0s - loss: 0.6104 - acc: 0.9833 - val_loss: 0.6003 - val_acc: 0.9333\n",
      "Epoch 185/250\n",
      "120/120 [==============================] - 0s - loss: 0.6077 - acc: 0.9833 - val_loss: 0.5973 - val_acc: 0.9000\n",
      "Epoch 186/250\n",
      "120/120 [==============================] - 0s - loss: 0.6051 - acc: 0.9833 - val_loss: 0.5949 - val_acc: 0.9000\n",
      "Epoch 187/250\n",
      "120/120 [==============================] - 0s - loss: 0.6031 - acc: 0.9833 - val_loss: 0.5931 - val_acc: 0.9333\n",
      "Epoch 188/250\n",
      "120/120 [==============================] - 0s - loss: 0.5996 - acc: 0.9833 - val_loss: 0.5898 - val_acc: 0.9000\n",
      "Epoch 189/250\n",
      "120/120 [==============================] - 0s - loss: 0.5980 - acc: 0.9750 - val_loss: 0.5871 - val_acc: 0.9000\n",
      "Epoch 190/250\n",
      "120/120 [==============================] - 0s - loss: 0.5945 - acc: 0.9833 - val_loss: 0.5849 - val_acc: 0.9000\n",
      "Epoch 191/250\n",
      "120/120 [==============================] - 0s - loss: 0.5922 - acc: 0.9833 - val_loss: 0.5840 - val_acc: 0.9333\n",
      "Epoch 192/250\n",
      "120/120 [==============================] - 0s - loss: 0.5899 - acc: 0.9833 - val_loss: 0.5810 - val_acc: 0.9333\n",
      "Epoch 193/250\n",
      "120/120 [==============================] - 0s - loss: 0.5870 - acc: 0.9833 - val_loss: 0.5775 - val_acc: 0.9000\n",
      "Epoch 194/250\n",
      "120/120 [==============================] - 0s - loss: 0.5849 - acc: 0.9833 - val_loss: 0.5750 - val_acc: 0.9000\n",
      "Epoch 195/250\n",
      "120/120 [==============================] - 0s - loss: 0.5819 - acc: 0.9833 - val_loss: 0.5733 - val_acc: 0.9333\n",
      "Epoch 196/250\n",
      "120/120 [==============================] - 0s - loss: 0.5802 - acc: 0.9833 - val_loss: 0.5705 - val_acc: 0.9333\n",
      "Epoch 197/250\n",
      "120/120 [==============================] - 0s - loss: 0.5767 - acc: 0.9833 - val_loss: 0.5685 - val_acc: 0.9333\n",
      "Epoch 198/250\n",
      "120/120 [==============================] - 0s - loss: 0.5751 - acc: 0.9833 - val_loss: 0.5658 - val_acc: 0.9333\n",
      "Epoch 199/250\n",
      "120/120 [==============================] - 0s - loss: 0.5726 - acc: 0.9833 - val_loss: 0.5648 - val_acc: 0.9333\n",
      "Epoch 200/250\n",
      "120/120 [==============================] - 0s - loss: 0.5699 - acc: 0.9833 - val_loss: 0.5612 - val_acc: 0.9333\n",
      "Epoch 201/250\n",
      "120/120 [==============================] - 0s - loss: 0.5673 - acc: 0.9833 - val_loss: 0.5589 - val_acc: 0.9000\n",
      "Epoch 202/250\n",
      "120/120 [==============================] - 0s - loss: 0.5650 - acc: 0.9833 - val_loss: 0.5570 - val_acc: 0.9333\n",
      "Epoch 203/250\n",
      "120/120 [==============================] - 0s - loss: 0.5624 - acc: 0.9833 - val_loss: 0.5547 - val_acc: 0.9333\n",
      "Epoch 204/250\n",
      "120/120 [==============================] - 0s - loss: 0.5603 - acc: 0.9833 - val_loss: 0.5521 - val_acc: 0.9000\n",
      "Epoch 205/250\n",
      "120/120 [==============================] - 0s - loss: 0.5578 - acc: 0.9833 - val_loss: 0.5500 - val_acc: 0.9333\n",
      "Epoch 206/250\n",
      "120/120 [==============================] - 0s - loss: 0.5553 - acc: 0.9833 - val_loss: 0.5478 - val_acc: 0.9333\n",
      "Epoch 207/250\n",
      "120/120 [==============================] - 0s - loss: 0.5537 - acc: 0.9833 - val_loss: 0.5456 - val_acc: 0.9333\n",
      "Epoch 208/250\n",
      "120/120 [==============================] - 0s - loss: 0.5506 - acc: 0.9833 - val_loss: 0.5433 - val_acc: 0.9333\n",
      "Epoch 209/250\n",
      "120/120 [==============================] - 0s - loss: 0.5489 - acc: 0.9833 - val_loss: 0.5411 - val_acc: 0.9000\n",
      "Epoch 210/250\n",
      "120/120 [==============================] - 0s - loss: 0.5463 - acc: 0.9833 - val_loss: 0.5388 - val_acc: 0.9333\n",
      "Epoch 211/250\n",
      "120/120 [==============================] - 0s - loss: 0.5437 - acc: 0.9833 - val_loss: 0.5369 - val_acc: 0.9333\n",
      "Epoch 212/250\n",
      "120/120 [==============================] - 0s - loss: 0.5416 - acc: 0.9833 - val_loss: 0.5351 - val_acc: 0.9333\n",
      "Epoch 213/250\n",
      "120/120 [==============================] - 0s - loss: 0.5399 - acc: 0.9833 - val_loss: 0.5330 - val_acc: 0.9333\n",
      "Epoch 214/250\n",
      "120/120 [==============================] - 0s - loss: 0.5370 - acc: 0.9833 - val_loss: 0.5305 - val_acc: 0.9333\n",
      "Epoch 215/250\n",
      "120/120 [==============================] - 0s - loss: 0.5353 - acc: 0.9833 - val_loss: 0.5285 - val_acc: 0.9000\n",
      "Epoch 216/250\n",
      "120/120 [==============================] - 0s - loss: 0.5330 - acc: 0.9833 - val_loss: 0.5264 - val_acc: 0.9333\n",
      "Epoch 217/250\n",
      "120/120 [==============================] - 0s - loss: 0.5312 - acc: 0.9833 - val_loss: 0.5245 - val_acc: 0.9333\n",
      "Epoch 218/250\n",
      "120/120 [==============================] - 0s - loss: 0.5292 - acc: 0.9833 - val_loss: 0.5223 - val_acc: 0.9333\n",
      "Epoch 219/250\n",
      "120/120 [==============================] - 0s - loss: 0.5268 - acc: 0.9833 - val_loss: 0.5202 - val_acc: 0.9333\n",
      "Epoch 220/250\n",
      "120/120 [==============================] - 0s - loss: 0.5247 - acc: 0.9833 - val_loss: 0.5185 - val_acc: 0.9667\n",
      "Epoch 221/250\n",
      "120/120 [==============================] - 0s - loss: 0.5220 - acc: 0.9833 - val_loss: 0.5162 - val_acc: 0.9333\n",
      "Epoch 222/250\n",
      "120/120 [==============================] - 0s - loss: 0.5207 - acc: 0.9833 - val_loss: 0.5144 - val_acc: 0.9000\n",
      "Epoch 223/250\n",
      "120/120 [==============================] - 0s - loss: 0.5185 - acc: 0.9833 - val_loss: 0.5122 - val_acc: 0.9667\n",
      "Epoch 224/250\n",
      "120/120 [==============================] - 0s - loss: 0.5161 - acc: 0.9833 - val_loss: 0.5104 - val_acc: 0.9667\n",
      "Epoch 225/250\n",
      "120/120 [==============================] - 0s - loss: 0.5142 - acc: 0.9833 - val_loss: 0.5083 - val_acc: 0.9333\n",
      "Epoch 226/250\n",
      "120/120 [==============================] - 0s - loss: 0.5123 - acc: 0.9833 - val_loss: 0.5063 - val_acc: 0.9333\n",
      "Epoch 227/250\n",
      "120/120 [==============================] - 0s - loss: 0.5100 - acc: 0.9833 - val_loss: 0.5045 - val_acc: 0.9333\n",
      "Epoch 228/250\n",
      "120/120 [==============================] - 0s - loss: 0.5085 - acc: 0.9833 - val_loss: 0.5026 - val_acc: 0.9333\n",
      "Epoch 229/250\n",
      "120/120 [==============================] - 0s - loss: 0.5057 - acc: 0.9833 - val_loss: 0.5005 - val_acc: 0.9667\n",
      "Epoch 230/250\n",
      "120/120 [==============================] - 0s - loss: 0.5052 - acc: 0.9833 - val_loss: 0.4999 - val_acc: 0.9667\n",
      "Epoch 231/250\n",
      "120/120 [==============================] - 0s - loss: 0.5025 - acc: 0.9833 - val_loss: 0.4969 - val_acc: 0.9667\n",
      "Epoch 232/250\n",
      "120/120 [==============================] - 0s - loss: 0.5004 - acc: 0.9833 - val_loss: 0.4957 - val_acc: 0.9000\n",
      "Epoch 233/250\n",
      "120/120 [==============================] - 0s - loss: 0.4998 - acc: 0.9833 - val_loss: 0.4935 - val_acc: 0.9333\n",
      "Epoch 234/250\n",
      "120/120 [==============================] - 0s - loss: 0.4992 - acc: 0.9833 - val_loss: 0.4925 - val_acc: 0.9667\n",
      "Epoch 235/250\n",
      "120/120 [==============================] - 0s - loss: 0.4949 - acc: 0.9833 - val_loss: 0.4896 - val_acc: 0.9667\n",
      "Epoch 236/250\n",
      "120/120 [==============================] - 0s - loss: 0.4929 - acc: 0.9833 - val_loss: 0.4883 - val_acc: 0.9333\n",
      "Epoch 237/250\n",
      "120/120 [==============================] - 0s - loss: 0.4911 - acc: 0.9833 - val_loss: 0.4860 - val_acc: 0.9667\n",
      "Epoch 238/250\n",
      "120/120 [==============================] - 0s - loss: 0.4892 - acc: 0.9833 - val_loss: 0.4842 - val_acc: 0.9667\n",
      "Epoch 239/250\n",
      "120/120 [==============================] - 0s - loss: 0.4870 - acc: 0.9833 - val_loss: 0.4825 - val_acc: 0.9667\n",
      "Epoch 240/250\n",
      "120/120 [==============================] - 0s - loss: 0.4854 - acc: 0.9833 - val_loss: 0.4810 - val_acc: 0.9333\n",
      "Epoch 241/250\n",
      "120/120 [==============================] - 0s - loss: 0.4834 - acc: 0.9833 - val_loss: 0.4790 - val_acc: 0.9667\n",
      "Epoch 242/250\n",
      "120/120 [==============================] - 0s - loss: 0.4817 - acc: 0.9833 - val_loss: 0.4772 - val_acc: 0.9667\n",
      "Epoch 243/250\n",
      "120/120 [==============================] - 0s - loss: 0.4802 - acc: 0.9833 - val_loss: 0.4756 - val_acc: 0.9667\n",
      "Epoch 244/250\n",
      "120/120 [==============================] - 0s - loss: 0.4787 - acc: 0.9833 - val_loss: 0.4738 - val_acc: 0.9667\n",
      "Epoch 245/250\n",
      "120/120 [==============================] - 0s - loss: 0.4766 - acc: 0.9833 - val_loss: 0.4724 - val_acc: 0.9667\n",
      "Epoch 246/250\n",
      "120/120 [==============================] - 0s - loss: 0.4747 - acc: 0.9833 - val_loss: 0.4704 - val_acc: 0.9667\n",
      "Epoch 247/250\n",
      "120/120 [==============================] - 0s - loss: 0.4727 - acc: 0.9833 - val_loss: 0.4691 - val_acc: 0.9667\n",
      "Epoch 248/250\n",
      "120/120 [==============================] - 0s - loss: 0.4727 - acc: 0.9833 - val_loss: 0.4672 - val_acc: 0.9667\n",
      "Epoch 249/250\n",
      "120/120 [==============================] - 0s - loss: 0.4704 - acc: 0.9833 - val_loss: 0.4665 - val_acc: 0.9333\n",
      "Epoch 250/250\n",
      "120/120 [==============================] - 0s - loss: 0.4684 - acc: 0.9833 - val_loss: 0.4638 - val_acc: 0.9667\n",
      "30/30 [==============================] - 0s\n",
      "\n",
      "acc: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim=4, activation='tanh'))\n",
    "model.add(Dense(3, activation='tanh', activity_regularizer=l2(l=0.01)))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=20, \n",
    "          nb_epoch=250, \n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sieć złożona jest z trzech warstw: wejściowa o rozmiarze 4, pierwsza środkowa - 6, druga ukryta - 3, oraz wyjściowa o rozmiarze 3. Skrótowo można to przedstawić jako:\n",
    "4 wejścia -> 6 ukrytych neuronów -> 3 ukryte neurony -> 3 wyjścia\n",
    "\n",
    "Funkcje aktywacji w dwóch pierwszych warstwach to tangens hiperboliczny, natomiast w ostatniej - sigmoid. Funkcja kosztu to categorical_crossentropy, optymalizator to Adam. Metoda regularyzacji: weight decay.\n",
    "\n",
    "Przy ustalaniu rozmiaru warstw sugerowałem się odpowiedzią z tego linku: https://www.researchgate.net/post/In_neural_networks_model_which_number_of_hidden_units_to_select.\n",
    "Cytując:\n",
    "\"it is saying that the optimal number of hidden nodes in the first hidden layer is: $\\sqrt{N*(m+2)} + 2*\\sqrt{\\frac{N}{m+2}}$ and in the second hidden layer, the optimal number of hidden nodes is: $m*\\sqrt{\\frac{N}{m+2}}$, where $N$ - number of inputs, and $m$ - number of outputs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 20s - loss: 0.2446 - acc: 0.9239 - val_loss: 0.1211 - val_acc: 0.9631\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 17s - loss: 0.1025 - acc: 0.9688 - val_loss: 0.0796 - val_acc: 0.9750\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 17s - loss: 0.0758 - acc: 0.9776 - val_loss: 0.0768 - val_acc: 0.9774\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 17s - loss: 0.0617 - acc: 0.9818 - val_loss: 0.0908 - val_acc: 0.9731\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 18s - loss: 0.0511 - acc: 0.9852 - val_loss: 0.0895 - val_acc: 0.9786\n",
      "Test score: 0.0895062428832\n",
      "Test accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do jakiej postaci sprowadzane są dane Y_train i Y_test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane są sprowadzone do 10-elementowych list z jedynką na i-tym miejscu, gdzie i odpowiada etykiecie (czyli jaka cyfra została zakodowana za pomocą tego zero-jedynkowego ciągu). W powyższym przykładu - jest to cyfra 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Przedstaw wzór matematyczny na zastosowaną funkcję błędu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zastosowana funkcja błędu to 'categorical_crossentropy', według dokumentacji jej wzór to: $ H(p,q) = - \\sum\\nolimits_x p(x)log(q(x)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jaka jest architektura sieci neuronowej? Ile ma warstw, jakie są rozmiary macierzy warstw? Czy można uzyskać dostęp do tych wag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warstwa wejściowa ma 784 neurony, dwie warstwy ukryte po 512 neuronów, natomiast warstwa wyjściowa - 10 neuronów.\n",
    "Dostęp do wag można uzyskać metodą get_weights():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.03033457, -0.0678953 ,  0.0155883 , ...,  0.05736679,\n",
      "        -0.03349178,  0.00315883],\n",
      "       [-0.01981754,  0.06150286,  0.00792126, ..., -0.00235655,\n",
      "         0.05680166,  0.0634056 ],\n",
      "       [-0.01277079,  0.01821516, -0.03614987, ...,  0.02509774,\n",
      "        -0.00286663, -0.0061179 ],\n",
      "       ..., \n",
      "       [-0.05567249, -0.01085892, -0.04554728, ..., -0.03360587,\n",
      "        -0.06234068,  0.03258257],\n",
      "       [ 0.06559348, -0.00229711,  0.0365497 , ..., -0.05713066,\n",
      "        -0.05771172,  0.00244217],\n",
      "       [-0.06791238, -0.00827733, -0.0396281 , ..., -0.0246511 ,\n",
      "        -0.02268424,  0.02794694]], dtype=float32), array([  9.79289180e-04,  -3.19409417e-03,  -5.96103445e-02,\n",
      "        -1.18616764e-02,  -2.61336695e-02,  -6.31945878e-02,\n",
      "         2.51488052e-02,  -5.32893315e-02,   3.19988467e-05,\n",
      "        -2.21749768e-03,  -3.18477899e-02,  -2.99981199e-02,\n",
      "        -3.27082276e-02,  -3.44694220e-02,   2.94932928e-02,\n",
      "        -4.43002433e-02,  -6.68200627e-02,  -4.42358516e-02,\n",
      "        -1.09409029e-02,   1.70480497e-02,  -2.99014449e-02,\n",
      "        -9.16440599e-03,  -4.42207158e-02,  -4.44180295e-02,\n",
      "        -2.58128345e-02,   4.66033854e-02,   1.28699122e-02,\n",
      "        -5.64913498e-03,  -2.03776080e-02,  -3.47159281e-02,\n",
      "        -1.36444448e-02,  -3.32102478e-02,  -2.93480512e-02,\n",
      "        -1.72272008e-02,  -4.89391536e-02,  -1.19203404e-02,\n",
      "        -3.49217169e-02,  -7.71704391e-02,  -3.30382623e-02,\n",
      "         5.18984627e-03,   3.61329643e-03,   1.61278173e-02,\n",
      "        -6.23991638e-02,  -4.12536897e-02,  -1.33404732e-02,\n",
      "        -3.64470743e-02,  -1.85351688e-02,  -2.74825823e-02,\n",
      "        -5.30967303e-02,  -7.15999752e-02,  -4.24973294e-02,\n",
      "        -3.54775228e-02,  -3.46176177e-02,  -4.87115122e-02,\n",
      "         4.67508147e-03,  -1.74733140e-02,  -3.40799205e-02,\n",
      "         6.82523381e-03,   1.26084462e-02,  -2.29813866e-02,\n",
      "        -1.84032191e-02,   3.10112126e-02,   1.25036566e-02,\n",
      "        -3.50453965e-02,  -3.29683274e-02,  -1.02789439e-02,\n",
      "        -1.26130711e-02,  -4.86952215e-02,  -5.94230816e-02,\n",
      "        -4.97864597e-02,  -6.31037429e-02,  -1.12405922e-02,\n",
      "        -1.97286587e-02,   3.13222669e-02,  -3.05060335e-02,\n",
      "        -3.40623856e-02,  -6.86275288e-02,  -2.28330344e-02,\n",
      "         6.78996649e-03,  -2.77934931e-02,   1.07225054e-03,\n",
      "        -2.15566810e-02,  -2.31645461e-02,   1.12007335e-02,\n",
      "        -1.22630205e-02,  -1.75571796e-02,  -8.18194374e-02,\n",
      "        -2.17300244e-02,  -8.31539277e-04,  -2.14905646e-02,\n",
      "         1.92960780e-02,  -8.75557493e-03,   1.32627040e-02,\n",
      "        -1.86156724e-02,  -1.56370159e-02,  -1.19633116e-02,\n",
      "        -2.48670876e-02,  -3.30198146e-02,  -5.64851351e-02,\n",
      "        -5.86102009e-02,   1.28788231e-02,  -6.05017990e-02,\n",
      "         4.29870710e-02,   2.82356399e-03,  -6.09562658e-02,\n",
      "        -6.44478295e-03,   2.48134192e-02,   2.50887014e-02,\n",
      "         1.61785837e-02,   4.41150070e-04,  -2.20733676e-02,\n",
      "        -1.81777030e-02,  -3.89094464e-02,  -1.71341375e-02,\n",
      "        -2.38998123e-02,  -2.15702746e-02,  -1.67462416e-02,\n",
      "         5.60006883e-04,  -2.40362603e-02,  -3.49373072e-02,\n",
      "         6.58995658e-03,  -1.08386735e-02,   1.22274384e-02,\n",
      "        -5.57934158e-02,  -3.26550566e-02,   3.52106290e-03,\n",
      "         2.95311026e-02,   1.65511742e-02,   3.08886450e-02,\n",
      "         1.40143204e-02,   3.19897640e-03,  -1.81625057e-02,\n",
      "        -3.52310836e-02,  -1.70738734e-02,  -7.95768853e-03,\n",
      "         2.03894079e-02,  -6.65163174e-02,  -1.48530193e-02,\n",
      "         2.00721947e-03,  -5.23376875e-02,  -7.28739752e-03,\n",
      "        -3.54651250e-02,  -5.71916811e-02,  -2.56767906e-02,\n",
      "         1.02462005e-02,  -1.21689327e-02,  -5.64056821e-03,\n",
      "        -2.14758329e-02,  -1.08238971e-02,  -2.19903644e-02,\n",
      "         2.76497453e-02,  -2.25391239e-02,  -4.02700007e-02,\n",
      "        -3.09378691e-02,  -5.97856343e-02,  -5.71072064e-02,\n",
      "        -1.56437624e-02,  -2.13957224e-02,  -2.56586596e-02,\n",
      "        -5.07053919e-02,  -1.40837142e-02,  -9.44331102e-03,\n",
      "        -6.34721145e-02,   2.17444170e-02,  -3.65870888e-03,\n",
      "        -1.17917554e-02,   1.04631446e-02,  -4.86168526e-02,\n",
      "        -3.97889391e-02,   2.08667596e-03,  -5.71831129e-02,\n",
      "        -7.45134288e-03,  -1.31321354e-02,  -1.36157230e-03,\n",
      "        -5.51177673e-02,   7.54844444e-03,  -4.00472209e-02,\n",
      "         2.99086552e-02,   9.05364403e-04,  -3.61193642e-02,\n",
      "        -4.52846400e-02,  -5.41820712e-02,  -8.68797861e-03,\n",
      "        -5.83218858e-02,  -1.72755532e-02,  -3.44386660e-02,\n",
      "        -3.96112241e-02,  -2.59063691e-02,  -9.17050093e-02,\n",
      "        -5.56213111e-02,  -2.76217777e-02,  -3.40871997e-02,\n",
      "        -5.25284559e-02,  -2.39065550e-02,  -7.25820288e-03,\n",
      "        -6.40398636e-02,  -3.20227407e-02,   6.88255057e-02,\n",
      "         3.37433219e-02,   6.07966334e-02,  -3.42741273e-02,\n",
      "        -5.34801073e-02,  -4.36418056e-02,  -5.74138910e-02,\n",
      "        -4.79163416e-02,  -5.56593463e-02,  -4.67377082e-02,\n",
      "        -1.98745579e-02,   4.63687070e-02,  -1.66563503e-02,\n",
      "        -3.92325316e-03,  -3.23047936e-02,  -4.53464761e-02,\n",
      "        -1.33905536e-03,  -5.31722493e-02,  -5.88654354e-02,\n",
      "        -1.40191074e-02,  -4.80794720e-02,  -1.38691068e-03,\n",
      "        -2.66970638e-02,  -3.87315564e-02,  -2.54920013e-02,\n",
      "        -5.79877049e-02,  -2.89016198e-02,  -3.47866863e-02,\n",
      "        -2.77637839e-02,  -1.24610402e-02,  -2.53459476e-02,\n",
      "        -3.49638462e-02,  -5.95806912e-02,  -5.78334592e-02,\n",
      "        -3.74458656e-02,  -5.30599616e-02,  -1.53352097e-02,\n",
      "        -2.25906944e-04,  -6.12454265e-02,  -4.02592234e-02,\n",
      "        -3.75828054e-03,   2.61250380e-02,  -1.11823268e-02,\n",
      "        -2.34615002e-02,  -8.26558992e-02,  -3.63825150e-02,\n",
      "        -2.86115073e-02,  -5.65366596e-02,   1.38192503e-02,\n",
      "        -7.26704970e-02,  -6.95992867e-03,   1.90413650e-02,\n",
      "        -5.14892228e-02,   7.12194992e-03,  -1.76337659e-02,\n",
      "        -2.66851168e-02,  -2.13024970e-02,   1.25939418e-02,\n",
      "        -8.82075876e-02,  -5.49502745e-02,  -6.42363168e-03,\n",
      "        -1.36368126e-02,  -3.20432298e-02,  -1.07615711e-02,\n",
      "        -5.26548270e-03,   4.16530576e-03,  -9.34003741e-02,\n",
      "        -3.03565115e-02,   2.58671008e-02,  -7.36686662e-02,\n",
      "        -2.78219860e-02,  -3.53219658e-02,  -4.18770351e-02,\n",
      "         1.66545026e-02,  -6.39615068e-03,  -2.28017736e-02,\n",
      "        -3.88923623e-02,  -5.29335737e-02,  -5.26698120e-02,\n",
      "        -3.03779580e-02,  -2.96090301e-02,  -5.55345677e-02,\n",
      "        -4.16677669e-02,  -3.71865481e-02,  -5.39918318e-02,\n",
      "        -1.19219385e-02,   6.42977702e-03,  -5.64142577e-02,\n",
      "        -6.25341982e-02,  -5.48275597e-02,  -9.32235178e-03,\n",
      "        -3.83059913e-03,   8.82394891e-03,  -3.26331742e-02,\n",
      "        -7.98679329e-03,  -2.55097151e-02,  -2.22988520e-02,\n",
      "        -4.05478068e-02,  -4.62287702e-02,   2.04264000e-02,\n",
      "         1.47458641e-02,  -1.49461757e-02,  -2.95489691e-02,\n",
      "        -6.48596212e-02,  -4.48598452e-02,  -4.09490578e-02,\n",
      "        -1.21020051e-02,  -3.25854458e-02,   2.22039744e-02,\n",
      "         1.32136671e-02,  -2.59635933e-02,  -7.09605440e-02,\n",
      "        -7.06361979e-02,  -9.72790737e-03,  -1.27330720e-02,\n",
      "         3.17641236e-02,  -4.14731838e-02,  -7.00039091e-05,\n",
      "        -5.05747683e-02,  -2.37363745e-02,   3.05145849e-02,\n",
      "        -6.65996149e-02,  -3.43291573e-02,   2.91968342e-02,\n",
      "        -3.49882618e-02,   6.07365184e-03,   1.82394125e-02,\n",
      "        -3.56545225e-02,  -3.54623161e-02,  -4.95833047e-02,\n",
      "        -2.98128072e-02,  -1.55655425e-02,  -3.13002393e-02,\n",
      "        -3.45809422e-02,  -4.40705754e-02,   7.84274749e-03,\n",
      "        -2.24828087e-02,  -1.11233499e-02,  -2.36097611e-02,\n",
      "        -6.62128534e-03,  -4.75053936e-02,  -1.21732857e-02,\n",
      "        -2.76114754e-02,  -1.72478966e-02,  -4.61877510e-02,\n",
      "        -3.55330594e-02,  -1.28726522e-02,  -3.75962034e-02,\n",
      "        -2.19680462e-02,   3.04102283e-02,  -1.94346514e-02,\n",
      "        -4.19908613e-02,  -6.44306168e-02,  -4.43548709e-02,\n",
      "        -1.64582487e-02,  -4.20789830e-02,  -3.11044622e-02,\n",
      "        -1.50417564e-02,  -3.02233789e-02,   2.28419229e-02,\n",
      "        -1.30720269e-02,  -8.76517892e-02,   1.06394598e-02,\n",
      "        -7.02781184e-03,  -3.26294526e-02,  -1.93974953e-02,\n",
      "        -4.36458923e-02,  -2.37942440e-03,   5.47460653e-03,\n",
      "        -5.48254475e-02,  -5.72641417e-02,  -4.16268408e-02,\n",
      "        -2.85766274e-02,   1.84556227e-02,   1.93986204e-02,\n",
      "        -5.76518774e-02,   1.73482765e-02,  -2.48107146e-02,\n",
      "        -4.13547829e-03,  -3.15746330e-02,   4.02938249e-03,\n",
      "        -3.74674089e-02,  -5.16133709e-03,  -3.00387945e-02,\n",
      "        -1.74561609e-02,  -5.35904393e-02,  -4.56229299e-02,\n",
      "         1.09598981e-02,   4.94656041e-02,  -4.78811041e-02,\n",
      "        -8.01958330e-03,  -5.07900603e-02,  -2.18934845e-02,\n",
      "         1.40832569e-02,  -4.10846137e-02,  -5.76806217e-02,\n",
      "         5.11542079e-04,  -7.15725049e-02,  -2.47559305e-02,\n",
      "        -2.44158483e-03,  -1.42130023e-02,  -2.29832642e-02,\n",
      "        -6.38097897e-02,  -4.67972867e-02,  -2.63507534e-02,\n",
      "        -1.92769300e-02,  -3.75533588e-02,  -1.71688851e-02,\n",
      "        -1.54546862e-02,  -2.04417855e-02,  -1.20419655e-02,\n",
      "        -2.18981523e-02,   1.16312539e-03,  -8.22364911e-03,\n",
      "        -3.63188013e-02,  -3.14324796e-02,  -4.70586680e-02,\n",
      "        -2.26592105e-02,  -5.51662780e-02,  -6.65285811e-03,\n",
      "        -5.33072762e-02,  -5.19072218e-03,  -1.90621596e-02,\n",
      "        -7.29200467e-02,  -2.04221513e-02,  -5.82275633e-03,\n",
      "        -5.39691560e-02,  -7.54787028e-02,  -3.82211655e-02,\n",
      "        -6.99886903e-02,  -2.89817937e-02,  -4.16323319e-02,\n",
      "        -4.51052282e-03,  -2.67731268e-02,  -3.19674201e-02,\n",
      "         1.31765204e-02,  -3.60008180e-02,  -5.25591224e-02,\n",
      "        -1.05262538e-02,  -3.58258300e-02,  -6.30637258e-02,\n",
      "        -2.62188986e-02,  -5.61766140e-03,  -3.00189294e-02,\n",
      "        -1.90034718e-03,  -4.23865654e-02,   4.06757835e-03,\n",
      "        -1.39293186e-02,  -9.67170461e-04,  -1.95366610e-02,\n",
      "        -6.32655770e-02,  -2.85160691e-02,  -2.72073038e-02,\n",
      "        -3.23664322e-02,  -5.69817331e-03,  -5.68068177e-02,\n",
      "        -3.44080888e-02,  -1.52500747e-02,  -1.21060815e-02,\n",
      "         2.30772793e-02,  -1.58647560e-02,  -9.40191001e-03,\n",
      "        -4.22000699e-02,  -2.71480903e-03,  -1.66288950e-02,\n",
      "        -7.87974745e-02,  -8.60762820e-02,   1.23716714e-02,\n",
      "        -7.07427263e-02,  -9.92950797e-03,  -5.79074174e-02,\n",
      "         1.35311829e-02,  -4.69797775e-02,  -4.33173142e-02,\n",
      "        -3.04073971e-02,  -2.66062841e-02,  -4.28862795e-02,\n",
      "         1.80719420e-02,  -2.94671319e-02,  -1.66270863e-02,\n",
      "         3.95674631e-02,  -1.08716665e-02,  -4.57889661e-02,\n",
      "        -3.71460989e-02,  -5.11067323e-02,  -1.01680942e-02,\n",
      "        -3.29753235e-02,  -2.41301209e-02,  -3.53981811e-03,\n",
      "        -1.75447036e-02,  -4.83752936e-02,  -2.75472030e-02,\n",
      "        -1.08814258e-02,  -6.53965995e-02,  -1.05016623e-02,\n",
      "        -1.34527190e-02,  -6.81102499e-02,  -2.80107092e-02,\n",
      "        -2.58541433e-03,  -3.36747281e-02,  -3.45915966e-02,\n",
      "         9.59955971e-04,  -1.43472515e-02,   3.13612726e-03,\n",
      "        -3.41572128e-02,  -1.56847741e-02,  -5.71934730e-02,\n",
      "        -3.27164829e-02,  -1.10157132e-02,   1.26721384e-02,\n",
      "        -4.06066030e-02,  -3.27387303e-02,  -1.22292936e-02,\n",
      "         3.27359773e-02,  -5.68334348e-02], dtype=float32)]\n",
      "[array([[-0.03476051,  0.01772583,  0.01691591, ...,  0.07398935,\n",
      "        -0.02656   , -0.13866386],\n",
      "       [ 0.06009692, -0.02002752, -0.07753325, ...,  0.13477637,\n",
      "         0.02803799,  0.00676065],\n",
      "       [-0.01598127, -0.13799959, -0.08339033, ...,  0.07502141,\n",
      "         0.01107955,  0.02976501],\n",
      "       ..., \n",
      "       [-0.10940597, -0.0311858 ,  0.1131124 , ...,  0.01361824,\n",
      "         0.13393506, -0.14231955],\n",
      "       [-0.06870631,  0.10334082,  0.07283226, ...,  0.10980577,\n",
      "        -0.11128673, -0.14646994],\n",
      "       [ 0.02235007, -0.04155836,  0.01275062, ...,  0.03961391,\n",
      "        -0.03747335,  0.08985854]], dtype=float32), array([ -3.58688459e-02,  -1.88899226e-02,   2.23401263e-02,\n",
      "        -5.17781675e-02,  -6.94157183e-02,   4.29403409e-03,\n",
      "        -6.62683621e-02,  -2.66806614e-02,  -1.17818721e-01,\n",
      "        -6.72565922e-02,  -5.84455766e-03,   4.85630298e-04,\n",
      "        -9.85910296e-02,  -6.24608509e-02,  -6.38031885e-02,\n",
      "         1.84103549e-02,  -5.89992758e-03,  -6.59182519e-02,\n",
      "        -9.56442431e-02,  -3.41923758e-02,  -1.47009427e-02,\n",
      "        -6.53866455e-02,  -1.54705932e-02,  -2.52238065e-02,\n",
      "        -1.70029178e-02,  -9.87561345e-02,  -6.63557053e-02,\n",
      "        -4.32146154e-02,  -7.15234280e-02,  -9.77917612e-02,\n",
      "        -2.51335390e-02,  -7.99668357e-02,  -5.20737916e-02,\n",
      "         1.67892892e-02,  -5.76386563e-02,  -4.48784493e-02,\n",
      "         5.61611450e-05,  -8.29105452e-02,  -1.22631872e-02,\n",
      "        -3.44026863e-04,  -6.97449520e-02,  -1.32825589e-02,\n",
      "        -6.93508936e-03,  -9.61557925e-02,  -6.36912435e-02,\n",
      "        -6.19992018e-02,  -3.49090211e-02,  -2.26002075e-02,\n",
      "        -4.47992831e-02,   1.17837312e-02,   4.51210663e-02,\n",
      "        -1.30634829e-02,  -2.53671072e-02,   1.15716625e-02,\n",
      "         6.20797416e-03,  -4.50041294e-02,   3.34798843e-02,\n",
      "        -1.82741359e-02,  -8.60497728e-02,  -9.42846388e-03,\n",
      "         2.64894925e-02,  -6.16585985e-02,   3.99070140e-03,\n",
      "        -2.50517465e-02,   3.64378490e-03,  -9.36829671e-03,\n",
      "         4.79451902e-02,  -1.00735314e-02,  -6.36737198e-02,\n",
      "        -2.08577476e-02,  -4.92996462e-02,  -2.10692100e-02,\n",
      "        -2.44584661e-02,  -2.25153640e-02,  -1.43886004e-02,\n",
      "        -8.16579759e-02,   1.01602310e-03,  -5.66101559e-02,\n",
      "        -9.16284882e-03,  -2.28600930e-02,   3.35510517e-03,\n",
      "        -6.66027218e-02,  -5.97681068e-02,  -4.58365530e-02,\n",
      "        -3.89291681e-02,  -6.26174808e-02,  -2.17486694e-02,\n",
      "        -1.16689727e-02,  -2.18514111e-02,  -1.19595513e-01,\n",
      "        -1.56210233e-02,  -5.65153360e-02,   2.35881545e-02,\n",
      "        -6.04695007e-02,  -3.44171226e-02,  -3.42422053e-02,\n",
      "        -7.22154370e-03,  -6.62555471e-02,  -3.27060185e-02,\n",
      "        -3.69446054e-02,  -8.59136730e-02,  -3.22851017e-02,\n",
      "        -4.04834151e-02,   1.93061656e-03,  -8.76707062e-02,\n",
      "        -5.43681532e-02,  -6.00446314e-02,  -5.89300580e-02,\n",
      "        -7.01972470e-02,  -8.61382391e-03,  -4.69957888e-02,\n",
      "        -5.37907481e-02,   2.09825803e-02,  -7.37956986e-02,\n",
      "        -2.58051492e-02,   3.76769565e-02,  -7.37865195e-02,\n",
      "        -6.82343468e-02,   1.71743985e-02,  -8.23390633e-02,\n",
      "        -7.35741258e-02,   3.94953191e-02,  -1.44974962e-02,\n",
      "        -1.63925830e-02,  -6.48136735e-02,  -4.22625057e-02,\n",
      "        -8.73199031e-02,  -5.61977290e-02,   9.94069967e-03,\n",
      "        -3.19793485e-02,  -6.76854327e-02,  -5.31308949e-02,\n",
      "        -4.89054620e-02,  -9.82163101e-03,  -5.22439182e-02,\n",
      "        -4.09167558e-02,  -7.52653107e-02,   4.49537449e-02,\n",
      "        -1.74438581e-02,  -4.88131717e-02,  -2.56371265e-03,\n",
      "        -1.42789269e-02,  -1.64345428e-02,  -2.58554798e-02,\n",
      "        -4.41266447e-02,  -4.66481261e-02,  -1.83056705e-02,\n",
      "        -2.42164135e-02,  -3.92107703e-02,  -2.62289401e-02,\n",
      "        -6.97414726e-02,  -2.27506440e-02,   1.83365010e-02,\n",
      "        -5.32558560e-02,  -9.82679278e-02,  -1.57824345e-02,\n",
      "        -4.02936004e-02,   4.12628502e-02,   1.41197005e-02,\n",
      "        -1.92907006e-02,  -6.84239268e-02,  -2.38303151e-02,\n",
      "        -3.94871868e-02,  -5.19042239e-02,   2.83117145e-02,\n",
      "        -2.94372719e-02,  -3.93105708e-02,  -1.44005036e-02,\n",
      "        -1.42934853e-02,  -9.56248567e-02,  -4.91907969e-02,\n",
      "        -2.96936389e-02,  -7.00445333e-03,  -5.08297351e-04,\n",
      "        -3.88010927e-02,  -5.79797514e-02,  -9.41478163e-02,\n",
      "         1.38786715e-02,  -1.70975849e-02,  -2.74972562e-02,\n",
      "        -1.14711851e-01,  -5.57392612e-02,  -6.42248020e-02,\n",
      "        -5.03534637e-02,  -9.05294493e-02,  -2.46026590e-02,\n",
      "        -5.47954179e-02,  -6.83639571e-03,  -3.64831612e-02,\n",
      "        -8.92701596e-02,   2.48769335e-02,  -3.54762077e-02,\n",
      "         6.47687586e-03,  -2.26871893e-02,  -2.11553201e-02,\n",
      "        -5.81579544e-02,   1.31092034e-03,  -1.43082708e-01,\n",
      "        -1.44899534e-02,  -6.50157109e-02,  -2.40831729e-03,\n",
      "        -3.42082940e-02,  -6.90118968e-02,  -9.36302841e-02,\n",
      "        -7.03122932e-03,  -3.29606719e-02,  -1.57563183e-02,\n",
      "        -8.35864544e-02,  -4.12573712e-03,   1.98848313e-03,\n",
      "         4.16008988e-03,  -5.31583615e-02,  -2.51362212e-02,\n",
      "        -2.55268905e-02,  -7.20128184e-03,   1.46760149e-02,\n",
      "        -6.06108941e-02,  -3.42712998e-02,  -3.84772122e-02,\n",
      "         7.03045679e-03,  -1.64716020e-02,  -3.85732129e-02,\n",
      "        -4.02684771e-02,  -1.42819490e-02,  -4.03727330e-02,\n",
      "        -4.73028980e-02,   3.59320939e-02,  -5.53632108e-03,\n",
      "        -6.02803044e-02,  -4.25478369e-02,  -1.38375477e-03,\n",
      "        -3.32249850e-02,  -1.13853645e-02,  -7.60790929e-02,\n",
      "        -5.43444082e-02,   1.35539072e-02,  -4.99302484e-02,\n",
      "         1.33826248e-02,  -6.33640289e-02,  -3.64150144e-02,\n",
      "         5.60141401e-03,  -3.42925601e-02,  -3.98971550e-02,\n",
      "        -6.26851544e-02,   3.35636064e-02,  -3.74915525e-02,\n",
      "        -6.64562313e-03,  -9.01985094e-02,  -4.75102924e-02,\n",
      "         1.06982607e-02,   2.84729097e-02,  -6.04282953e-02,\n",
      "         1.77695360e-02,  -3.98102477e-02,  -3.98214087e-02,\n",
      "        -4.77005076e-03,  -1.67012345e-02,  -1.18425198e-01,\n",
      "         5.40147237e-02,  -5.86528778e-02,  -4.35529239e-02,\n",
      "        -4.39014621e-02,  -3.36040594e-02,  -1.02292597e-01,\n",
      "         2.28081886e-02,   1.77906696e-02,  -1.44785773e-02,\n",
      "        -4.00560610e-02,  -3.05266678e-03,  -7.18341321e-02,\n",
      "        -6.95377886e-02,  -3.57782617e-02,  -6.25323653e-02,\n",
      "        -2.33945735e-02,  -4.76333238e-02,  -3.53920199e-02,\n",
      "         1.84954493e-03,  -1.52463987e-02,  -3.95437479e-02,\n",
      "        -2.59336531e-02,  -1.01877354e-01,  -7.72782713e-02,\n",
      "        -1.17063455e-01,  -9.31732059e-02,  -4.29732054e-02,\n",
      "        -7.82556608e-02,  -2.99156178e-02,  -6.73950389e-02,\n",
      "        -8.06441531e-02,  -1.99165810e-02,  -6.38212413e-02,\n",
      "        -4.09066305e-02,  -1.89134106e-02,  -5.47429249e-02,\n",
      "        -2.23616399e-02,   2.60691214e-02,  -7.68308118e-02,\n",
      "        -3.03596277e-02,  -1.51268728e-02,  -4.77091335e-02,\n",
      "        -2.41255742e-02,  -6.18442520e-02,  -2.63944408e-03,\n",
      "        -2.36600693e-02,  -6.69363886e-02,  -4.05909754e-02,\n",
      "        -1.90556757e-02,  -2.03516092e-02,  -3.57193910e-02,\n",
      "         5.48297632e-03,   6.57999946e-04,  -3.52945104e-02,\n",
      "        -1.92838535e-02,  -8.95693991e-03,  -3.63590568e-02,\n",
      "        -4.54870947e-02,  -3.52595299e-02,  -5.63501641e-02,\n",
      "        -6.56920895e-02,  -4.15186994e-02,  -2.39060726e-02,\n",
      "        -4.33705412e-02,  -4.32543643e-03,  -4.98766489e-02,\n",
      "        -7.35961944e-02,  -3.72830629e-02,  -7.73983076e-02,\n",
      "         1.05498666e-02,  -3.46934684e-02,  -5.45119159e-02,\n",
      "        -6.57177642e-02,  -2.93008727e-03,  -2.69340165e-03,\n",
      "        -4.67259735e-02,  -8.14554691e-02,  -1.68085564e-02,\n",
      "        -1.04098208e-02,  -7.03167096e-02,  -4.33053151e-02,\n",
      "        -6.10856339e-02,  -5.80308326e-02,  -2.98988502e-02,\n",
      "        -4.02511545e-02,   1.64196938e-02,  -1.12047037e-02,\n",
      "        -5.47991209e-02,  -3.26970965e-02,  -6.64598867e-02,\n",
      "         2.43291520e-02,  -1.41560584e-02,  -4.01524045e-02,\n",
      "        -3.72077934e-02,   3.31387343e-03,  -8.22596774e-02,\n",
      "        -4.82279658e-02,  -5.99944741e-02,  -1.01969175e-01,\n",
      "         1.44314403e-02,  -3.41394357e-02,  -2.48783603e-02,\n",
      "        -3.98665890e-02,   2.08444055e-02,  -6.48190156e-02,\n",
      "         1.95948817e-02,  -3.71936746e-02,  -3.86170000e-02,\n",
      "        -7.50835389e-02,  -1.01506747e-01,  -3.06671485e-04,\n",
      "        -9.50457295e-04,  -1.71707142e-02,   1.71212833e-02,\n",
      "        -2.52273958e-02,  -4.29411530e-02,   6.81331903e-02,\n",
      "        -4.23511490e-02,  -8.59796926e-02,  -5.45719489e-02,\n",
      "         6.72025839e-03,  -8.00787657e-02,   3.64369294e-03,\n",
      "        -2.99844556e-02,  -3.91327813e-02,  -5.97252585e-02,\n",
      "         1.97857320e-02,  -3.70660014e-02,  -7.01111332e-02,\n",
      "        -6.37646690e-02,  -8.68590176e-03,  -7.92408586e-02,\n",
      "        -6.11570366e-02,  -7.26181194e-02,  -2.06229016e-02,\n",
      "         3.30168605e-02,   1.91274378e-03,  -2.63561606e-02,\n",
      "        -3.38409133e-02,  -5.35585880e-02,  -3.69693749e-02,\n",
      "        -9.03450921e-02,  -2.57409457e-02,  -7.08813816e-02,\n",
      "        -9.25816819e-02,  -4.14917208e-02,   2.47519463e-02,\n",
      "         4.34289500e-03,  -3.88781615e-02,  -3.62008065e-02,\n",
      "        -6.94381446e-02,   3.13124922e-03,  -6.10223822e-02,\n",
      "         1.43111367e-02,  -4.52395193e-02,  -3.87178771e-02,\n",
      "         2.86640087e-03,  -7.73887038e-02,  -3.91193032e-02,\n",
      "        -3.26596238e-02,   8.54501966e-03,  -4.48005833e-02,\n",
      "        -1.18389837e-02,  -3.88419032e-02,  -5.93284257e-02,\n",
      "        -6.62699342e-02,  -4.31513488e-02,  -3.04199401e-02,\n",
      "        -3.08946371e-02,  -4.85924669e-02,  -5.82618862e-02,\n",
      "        -1.43193128e-03,  -5.55581376e-02,  -5.52018955e-02,\n",
      "        -8.48225504e-02,  -8.28707814e-02,   1.37380762e-02,\n",
      "        -1.06376605e-02,  -5.19545339e-02,  -1.32347140e-02,\n",
      "        -1.57609098e-02,  -2.16674171e-02,   2.02584881e-02,\n",
      "        -5.17596565e-02,  -8.38071927e-02,  -3.25617269e-02,\n",
      "        -9.91317183e-02,  -1.24704897e-01,  -1.59199461e-02,\n",
      "        -5.80138639e-02,  -1.69267282e-02,  -6.14097230e-02,\n",
      "         2.51253471e-02,  -5.56064695e-02,   4.05898551e-03,\n",
      "         2.14617457e-02,  -6.09273650e-02,  -6.70642033e-02,\n",
      "        -3.53248008e-02,  -4.47743423e-02,  -5.46501242e-02,\n",
      "         2.96981004e-03,  -5.29993176e-02,  -7.26384521e-02,\n",
      "        -4.41632532e-02,  -6.10541962e-02,  -8.80463421e-03,\n",
      "        -5.28421551e-02,  -3.20151374e-02,  -2.35872641e-02,\n",
      "        -2.00758106e-03,  -3.27374786e-02,  -1.41906645e-03,\n",
      "        -5.67039177e-02,  -3.93686071e-02,  -1.79408006e-02,\n",
      "        -1.17857102e-02,  -2.23454498e-02,  -4.25603613e-02,\n",
      "        -6.45661205e-02,  -6.39205333e-03,  -3.33114453e-02,\n",
      "        -3.95689383e-02,  -5.71282543e-02,  -3.90944108e-02,\n",
      "        -7.14209005e-02,  -5.63266426e-02,  -8.35761204e-02,\n",
      "        -9.06099975e-02,   6.17039623e-03,  -8.63066129e-03,\n",
      "        -6.98919445e-02,  -1.42635144e-02,  -6.15426004e-02,\n",
      "        -3.99374403e-02,  -6.97252676e-02,  -4.39633876e-02,\n",
      "        -8.67903456e-02,  -9.25003961e-02,  -4.60095443e-02,\n",
      "        -5.86215854e-02,  -4.62354794e-02,  -3.11029647e-02,\n",
      "        -5.23912087e-02,  -5.51885739e-02,   1.82326492e-02,\n",
      "        -6.57290071e-02,  -9.56339762e-02,  -1.49683598e-02,\n",
      "        -4.20986824e-02,  -3.92886885e-02,  -9.48511530e-03,\n",
      "        -8.53945464e-02,  -4.87317406e-02], dtype=float32)]\n",
      "[array([[-0.02093577,  0.03330028, -0.16731632, ..., -0.12638849,\n",
      "        -0.05159657, -0.01240475],\n",
      "       [-0.21067408, -0.05816917, -0.01763681, ...,  0.07931706,\n",
      "        -0.0312075 , -0.34649259],\n",
      "       [-0.187177  ,  0.00961969,  0.07598741, ...,  0.07826447,\n",
      "         0.08368453, -0.14819825],\n",
      "       ..., \n",
      "       [ 0.05215745, -0.1744419 , -0.08421269, ...,  0.02741563,\n",
      "         0.0722055 , -0.09735569],\n",
      "       [-0.12217262, -0.22931819,  0.0658275 , ..., -0.05687436,\n",
      "        -0.12686729, -0.02583306],\n",
      "       [-0.1338407 ,  0.00737245, -0.19507051, ..., -0.07181565,\n",
      "         0.05697339,  0.05672616]], dtype=float32), array([-0.01079593, -0.04453658, -0.03508899, -0.00909208, -0.01893284,\n",
      "        0.00157142, -0.04731821, -0.02454751,  0.08822491,  0.0187923 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if weights:\n",
    "        print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 512\n",
      "512 512\n",
      "512 10\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if weights:\n",
    "        print(len(weights[0]), len(weights[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macierze wag mają więc rozmiary: 784x512, 512x512 oraz 512x10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jakie funkcje aktywacji użyto? Podaj ich wzory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyto funkcje: ReLU oraz softmax (dla ostatniej warstwy). Wzory:\n",
    "* ReLU:\n",
    "$$ \\textrm{ReLU}(x) = \\max (0,x)$$\n",
    "\n",
    "* Softmax:\n",
    "$$ \\textrm{softmax}(k,x_1,\\dots,x_n) = \\dfrac{e^{x_k}}{\\sum_{i=i}^{n}e^{x_i}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Czym jest Dropout? Czemu służy? Jakie znaczenie ma parametr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout zapobiega overfittingowi sieci. Oznacza jaka cześć danych ma być ustawiona na zero w czasie aktualizacji podczas treningu (czyli jaka część neuronów ma być tymczasowo usunięta z sieci)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usuń warstwy Dropout, jaki jest efekt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 16s - loss: 0.2206 - acc: 0.9315 - val_loss: 0.1024 - val_acc: 0.9657\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s - loss: 0.0819 - acc: 0.9753 - val_loss: 0.0782 - val_acc: 0.9764\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 16s - loss: 0.0541 - acc: 0.9839 - val_loss: 0.0728 - val_acc: 0.9799\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 14s - loss: 0.0388 - acc: 0.9878 - val_loss: 0.0826 - val_acc: 0.9783\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 16s - loss: 0.0299 - acc: 0.9905 - val_loss: 0.0870 - val_acc: 0.9778\n",
      "Test score: 0.0870450309741\n",
      "Test accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po usunięciu parametru Dropout nie zauważyłem dużych zmian. Prawdopodobnie ze względu na zbyt małą liczbę epok - sieć nie zdążyła się przetrenować. :D Accuracy minimalnie niższe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stwórz 6-cio warstwowy model o rozmiarach warstw 2500, 2000, 1500, 1000, 500 oraz 10 bez Dropout, użyj wszędzie funkcji aktywacji tanh z wyjątkiem ostatniej warstwy, gdzie należy użyć softmax. Trenuj model przez 10 epok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 218s - loss: 0.9557 - acc: 0.7528 - val_loss: 0.3427 - val_acc: 0.9043\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 223s - loss: 0.2515 - acc: 0.9274 - val_loss: 0.1814 - val_acc: 0.9460\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 234s - loss: 0.1710 - acc: 0.9503 - val_loss: 0.1649 - val_acc: 0.9519\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 231s - loss: 0.1321 - acc: 0.9605 - val_loss: 0.1370 - val_acc: 0.9590\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 223s - loss: 0.1063 - acc: 0.9683 - val_loss: 0.1272 - val_acc: 0.9628\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 223s - loss: 0.0887 - acc: 0.9734 - val_loss: 0.1151 - val_acc: 0.9669\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 217s - loss: 0.0751 - acc: 0.9774 - val_loss: 0.1150 - val_acc: 0.9661\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 214s - loss: 0.0658 - acc: 0.9793 - val_loss: 0.1088 - val_acc: 0.9693\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 224s - loss: 0.0557 - acc: 0.9831 - val_loss: 0.1077 - val_acc: 0.9699\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 228s - loss: 0.0494 - acc: 0.9849 - val_loss: 0.1054 - val_acc: 0.9714\n",
      "Test score: 0.105382273719\n",
      "Test accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2500, input_shape=(784,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(2000))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1500))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=10,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 224s - loss: 0.7252 - acc: 0.8014 - val_loss: 0.2373 - val_acc: 0.9312\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 223s - loss: 0.2293 - acc: 0.9339 - val_loss: 0.2002 - val_acc: 0.9429\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 221s - loss: 0.1705 - acc: 0.9517 - val_loss: 0.1650 - val_acc: 0.9532\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 220s - loss: 0.1367 - acc: 0.9594 - val_loss: 0.1519 - val_acc: 0.9571\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 221s - loss: 0.1162 - acc: 0.9664 - val_loss: 0.1234 - val_acc: 0.9652\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 221s - loss: 0.1021 - acc: 0.9703 - val_loss: 0.1101 - val_acc: 0.9699\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 220s - loss: 0.0897 - acc: 0.9735 - val_loss: 0.1059 - val_acc: 0.9699\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 221s - loss: 0.0795 - acc: 0.9765 - val_loss: 0.0927 - val_acc: 0.9731\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 221s - loss: 0.0713 - acc: 0.9789 - val_loss: 0.1000 - val_acc: 0.9718\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 220s - loss: 0.0644 - acc: 0.9808 - val_loss: 0.1098 - val_acc: 0.9711\n",
      "Test score: 0.109812639213\n",
      "Test accuracy: 0.9711\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2500, input_shape=(784,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2000))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1500))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=10,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mniejsze wahania accuracy niż w przypadku braku parametru Dropdown. Acc rośnie wolniej, ale stabilniej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 207s - loss: 0.0864 - acc: 0.9750 - val_loss: 0.0932 - val_acc: 0.9734\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 204s - loss: 0.0399 - acc: 0.9880 - val_loss: 0.0835 - val_acc: 0.9775\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 202s - loss: 0.0277 - acc: 0.9916 - val_loss: 0.0834 - val_acc: 0.9783\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 202s - loss: 0.0210 - acc: 0.9938 - val_loss: 0.0837 - val_acc: 0.9781\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 203s - loss: 0.0165 - acc: 0.9954 - val_loss: 0.0888 - val_acc: 0.9785\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 203s - loss: 0.0138 - acc: 0.9959 - val_loss: 0.0888 - val_acc: 0.9787\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 204s - loss: 0.0109 - acc: 0.9969 - val_loss: 0.0893 - val_acc: 0.9788\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 212s - loss: 0.0094 - acc: 0.9973 - val_loss: 0.0902 - val_acc: 0.9788\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 214s - loss: 0.0087 - acc: 0.9974 - val_loss: 0.0947 - val_acc: 0.9787\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 207s - loss: 0.0076 - acc: 0.9981 - val_loss: 0.0942 - val_acc: 0.9795\n",
      "Test score: 0.0941757348379\n",
      "Test accuracy: 0.9795\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adagrad\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adagrad(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=10,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Już w pierwszej epoce accuracy wyniosło powyżej 90%, gdzie w poprzednich dwóch modelach (RMSprop z Dropout i bez) w tej samej epoce accuracy było na poziomie 75-80%. Acc w każdej kolejnej epoce sukcesywnie rośnie, by ostatecznie dać najlepsze dotychczas dopasowanie na zbiorze testowym."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
